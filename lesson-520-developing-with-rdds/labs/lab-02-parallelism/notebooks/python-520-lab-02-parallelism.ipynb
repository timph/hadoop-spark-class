{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing with RDDs:  Parallelism\n",
    "\n",
    "In this lab, you will take your first steps using RDDs in Spark using Jupyter Notebook.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Create the `SparkContext` to bootstrap `RDD`s.\n",
    "2. Use the `RDD`s to explore parallelism & its benefits when processing data.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This lab assumes that the student is familiar with the course environment, in particular, Jupyter Notebook.\n",
    "\n",
    "### Consider parallelism\n",
    "\n",
    "Whenever we process big data, we need to consider the degree to which we'll be able to use parallelism.  The general idea is that if we can break our job down into smaller and smaller chunks that can execute in parallel, we should perform better.  In this lab, we're going to test that theory by methodically increasing the degree of parallelism when performing a word count calculation to get the most frequently used word in some text file.\n",
    "\n",
    "### Get a SparkSession\n",
    "\n",
    "In order to work with Spark's SQL support, we need to first get our hands on a special context called `SparkSession`.  \n",
    "The SparkSession class is the entry point into all functionality in Spark. \n",
    "\n",
    "> Note: as of Spark 2.0, SparkSession replaced SqlContext. However, we could still use SqlContext as it's being kept for backward compatibility.\n",
    "\n",
    "We'll use SparkSession.builder to create a SparkSession. SparkSession.builder lets you define you application name and it also lets you set various parameters in the Spark config, although there is no need to do so for our simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark Parallelism\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read external file into RDD\n",
    "\n",
    "You'll be using a file sherlock-holmes.txt. You need to retrieve it from the container folder (/home/jovyan/Resources) mounted to host machine directory - see instructions about setting up the Docker container to run Jupyter Notebook.\n",
    "\n",
    "Read text into RDD. \n",
    "\n",
    "Make sure that the file was read correctly by printing the RDD element count (here the number of lines in the file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = \"/home/jovyan/Resources/sherlock-holmes.txt\"\n",
    "\n",
    "lines = spark.sparkContext.textFile(file)\n",
    "\n",
    "print(lines.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the words\n",
    "\n",
    "Before we get to controlling the degree of parallelism let's make sure we're able to count the words and find the one with the maximum count.\n",
    "\n",
    "Split the lines into individual words, for each word make a tuple, where the first element is the word itself and the second element is the initial count 1. Finally reduce by key obtaining the RDD with unique words and the total counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "import re\n",
    "\n",
    "words = lines.flatMap(lambda w : re.split('[ \\],.:;?!\\-@#\\(\\)\\\\\\*\\\"\\/]+', w)).map(lambda w : w.lower())\n",
    "\n",
    "dict = words.map(lambda w : (w, 1))\n",
    "counts = dict.reduceByKey(add)\n",
    "\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the word with the maximum count\n",
    "\n",
    "\n",
    "Let's define the function first which takes two tuples (word and count) as arguments and returns the one with the bigger count.\n",
    "\n",
    "Use the function to reduce the RDD to obtain the word with the biggest count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMax(r, c):\n",
    "  if (r[1] > c[1]):\n",
    "    return r\n",
    "  else:\n",
    "    return c\n",
    "\n",
    "max = counts.reduce(lambda r, c: getMax(r, c))\n",
    "\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control the degree of parallelism\n",
    "\n",
    "Until now we'have been doing a fairly straightforward word count, returning the most frequently used word. \n",
    "\n",
    "Now we want to execute the procedure in a loop, each time increasing the number of partitions participating in the execution thus increasing the level of parallelism.\n",
    "\n",
    "This will be accomplished by repartitioning the original RDD with cached lines of text through transforming it to another RDD with `partitionCount` partitions.\n",
    "\n",
    "For any RDD the number of partitions is given by `getNumPartitions`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partitionCount in range(2, 9, 2):\n",
    "\n",
    "    rept = lines.repartition(partitionCount)\n",
    "    print(rept.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After repartitioning we follow with the word counting procedure using the initial RDD after transformation.\n",
    "\n",
    "We want to capture the execution time in millisecods after each iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "What do you notice about our timings?  Do they confirm or refute our hypothesis that more parallism generally means better performance?  If so, great!  If not, what do you suppose are other factors that are influencing our outcome?  Discuss with your instructor and classmates!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import re\n",
    "\n",
    "# get the word with bigger count\n",
    "def getMax(r, c):\n",
    "  if (r[1] > c[1]):\n",
    "    return r\n",
    "  else:\n",
    "    return c\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark Parallelism\").getOrCreate()\n",
    "\n",
    "file = \"/home/jovyan/Resources/sherlock-holmes.txt\"\n",
    "\n",
    "# read lines and cache\n",
    "lines = spark.sparkContext.textFile(file).persist()\n",
    "\n",
    "minCores = 2\n",
    "numCores = 8\n",
    "\n",
    "# iterate over even number of partitions from minCores to numCores\n",
    "for partitionCount in range(minCores, numCores+1, 2):\n",
    "\n",
    "    rept = lines.repartition(partitionCount)\n",
    "\n",
    "    dt1 = datetime.now()\n",
    "    words = lines.flatMap(lambda w : re.split('[ \\],.:;?!\\-@#\\(\\)\\\\\\*\\\"\\/]*', w)).map(lambda w : w.lower())\n",
    "    dict = words.map(lambda w : (w, 1))\n",
    "    counts = dict.reduceByKey(add)\n",
    "    max = counts.reduce(lambda r, c: getMax(r, c))\n",
    "    dt2 = datetime.now()\n",
    "    \n",
    "    print(str(max) + \" partition count \" + str(partitionCount) + \" time \" + str(round((dt2-dt1).microseconds/1000)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
