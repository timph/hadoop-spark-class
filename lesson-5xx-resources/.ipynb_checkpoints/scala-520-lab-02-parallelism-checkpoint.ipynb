{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing with RDDs:  Parallelism\n",
    "\n",
    "In this lab, you will take your first steps using RDDs in Spark using the Spark REPL inside a Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "1. Use the `SparkContext` to bootstrap `RDD`s.\n",
    "2. Use the `RDD`s to explore parallelism & its benefits when processing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Consider parallelism\n",
    "\n",
    "Whenever we process big data, we need to consider the degree to which we'll be able to use parallelism.  The general idea is that if we can break our job down into smaller and smaller chunks that can execute in parallel, we should perform better.  In this lab, we're going to test that theory by methodically increasing the degree of parallelism when performing a word count calculation to get the most frequently used word in some text file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify directory containing source data\n",
    "\n",
    "Check with your course instructor to get the directory containing the source data for this lab.  It's probably at `../Resources` relative to this notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add code to control the degree of parallelism\n",
    "\n",
    "Below is a set of Scala code. This code is your starting point. We will play around with this algorithm in the execution cell, but so that you have the starting point, we'll kept it as a markdown as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "// NOTE: set the value srcDir to a string containing the absolute path of\n",
    "// the location of the source data directory, lesson-5xx-resources!\n",
    "val srcDir = \"../Resources\"\n",
    "val minCores = 2 // if your system has 2+ cores\n",
    "val nCores = 8 // depends on your system\n",
    "val partitions = 2 to nCores by 2\n",
    "val files = Array(\"byu-corpus.txt\")\n",
    "val delimiters = Array(' ',',','.',':',';','?','!','-','@','#','(',')')\n",
    "files foreach( file => {\n",
    "  val original = sc.textFile(srcDir + \"/\" + file);\n",
    "  println(\"File: \" + file);\n",
    "  partitions foreach(n => {\n",
    "    println(\"Partitions: \" + n)\n",
    "\n",
    "    // TODO: repartition the RDD to use n cores\n",
    "    val partitioned = ???;\n",
    "\n",
    "    val start = System.currentTimeMillis\n",
    "\n",
    "    // TODO: find the word used the most in the file using RDD called \"partitioned\"\n",
    "    val result = partitioned.???\n",
    "\n",
    "    val time = System.currentTimeMillis - start\n",
    "    println(\"Took: \" + time + \" ms\")\n",
    "  })\n",
    "})\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Cell...\n",
    "Here is where you can play with the algorithm. You have to read on to understand what we want you to do though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// NOTE: set the value srcDir to a string containing the absolute path of\n",
    "// the location of the source data directory, lesson-5xx-resources!\n",
    "val srcDir = \"../Resources\"\n",
    "val minCores = 2 // if your system has 2+ cores\n",
    "val nCores = 8 // depends on your system\n",
    "val partitions = 2 to nCores by 2\n",
    "val files = Array(\"byu-corpus.txt\")\n",
    "val delimiters = Array(' ',',','.',':',';','?','!','-','@','#','(',')')\n",
    "files foreach( file => {\n",
    "  val original = sc.textFile(srcDir + \"/\" + file);\n",
    "  println(\"File: \" + file);\n",
    "  partitions foreach(n => {\n",
    "    println(\"Partitions: \" + n)\n",
    "\n",
    "    // TODO: repartition the RDD to use n cores\n",
    "    val partitioned = ???\n",
    "\n",
    "    val start = System.currentTimeMillis\n",
    "\n",
    "    // TODO: find the word used the most in the file using RDD called \"partitioned\"\n",
    "    val result = ???\n",
    "      \n",
    "    val time = System.currentTimeMillis - start\n",
    "    println(\"Took: \" + time + \" ms\")\n",
    "  })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the line beginning with `val original = sc.textFile(srcDir + \"/\" + file);`.  That gives us an `RDD` with a default number of partitions, which is directly related to the degree of parallel computation that this `RDD` has.\n",
    "\n",
    "Next, look at the line following the first `TODO` comment; it begins with `val partitioned = ` and is left unimplemented.  Notice that this code is encased in a `foreach` loop that varies in the variable `n` the number of partitions under test.  \n",
    "\n",
    "Replace `???` with a call to `original.repartition(n)`, which will cause Spark to create a new `RDD` with the given number of partitions, which effectively determine the number of cores that we'll be executing on in this standalone example.  Your line should look like the following.\n",
    "\n",
    "``` scala\n",
    "val partitioned = original.repartition(n);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the word count `RDD` code\n",
    "\n",
    "Now that we are iterating on the parallelism of our `RDD`, we need to actually do something that's going to leverage it.  This lab will use the quintessential big data problem known as \"word count\" to find the word used the most in a given file of text.\n",
    "\n",
    "Look for the second `TODO` comment.  Below it, you'll see the line `var result = partitioned.???`.  Replace it with our word count code, given below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` scala\n",
    "val result = partitioned\n",
    "    .flatMap(_.split(delimiters))\n",
    "    .filter(_.trim.length > 0)\n",
    "    .map(x => (x, 1))\n",
    "    .reduceByKey(_ + _)\n",
    "    .sortBy(_._2, false)\n",
    "    .first;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we're doing a fairly straightforward word count and returning the most frequently used word.  The difference is that in each iteration, we're increasing the degree of parallelism that's used each time we calculate it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute and observe timings\n",
    "\n",
    "We're ready to roll now.  Execute the Spark script by running the execution cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` scala\n",
    "minCores: Int = 2\n",
    "nCores: Int = 8\n",
    "partitions: scala.collection.immutable.Range = Range(2, 4, 6, 8)\n",
    "files: Array[String] = Array(byu-corpus.txt)\n",
    "delimiters: Array[Char] = Array( , ,, ., :, ;, ?, !, -, @, #, (, ))\n",
    "File: byu-corpus.txt\n",
    "Partitions: 2\n",
    "Took: 5848 ms\n",
    "Partitions: 4\n",
    "Took: 3601 ms\n",
    "Partitions: 6\n",
    "Took: 3580 ms\n",
    "Partitions: 8\n",
    "Took: 2979 ms\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "What do you notice about our timings?  Do they confirm or refute our hypothesis that more parallelism generally means better performance?  If so, great!  If not, what do you suppose are other factors that are influencing our outcome?  Discuss with your instructor and classmates!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
