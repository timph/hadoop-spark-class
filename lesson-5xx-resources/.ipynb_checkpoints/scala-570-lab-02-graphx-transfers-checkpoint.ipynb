{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark GraphX\n",
    "\n",
    "In this lab, you will take your first steps using Spark's graph processing library, GraphX, specifically, using the PageRank algorithm on data comprised of financial transfers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Use the GraphX library to perform a PageRank scoring of financial transfer data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this lab, we're going to pretend that we're working in a ficticious company that provides the ability to transfer funds among its members.  Our business intelligence department wants us to identify customers who\n",
    "\n",
    "* are bringing money into their account so that we can reward them because we make money on the interest while they have money in their account,\n",
    "* are transferring money out of their account so that we can incentivize them to keep money in their account,\n",
    "* are using the platform a lot so we can learn more about them to guide new feature development, and\n",
    "* are deemed important, power users of the platform by virtue of their transfer activity.\n",
    "\n",
    "It sounds like a lot, but thanks to Spark's GraphX API, we can answer these questions quite easily.\n",
    "\n",
    "Our data is very simple:  each account is identified by a 4-digit account number from 1000-9999, and we simply have a record over some time period of transfers among those accounts in the form\n",
    "\n",
    "``` \n",
    "sourceAccount,destinationAccount,amount\n",
    "```\n",
    "\n",
    "This data naturally forms a directed multigraph that allows us to answer the questions that our BI department is asking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read transfer data\n",
    "\n",
    "The first thing we need to do is to read our transfer data, which will provide the basis of our graph.  Here's a small sample of the transfer data:\n",
    "\n",
    "``` \n",
    "5692,2555,289.32\n",
    "8691,8476,341.76\n",
    "6683,1083,76.54\n",
    "5202,1690,283.05\n",
    "5449,4779,741.42\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column identifies the source account, the second, the destination account, and the third, the amount of money transferred which we've constrained to between $1.00 and $1,000.00 for this lab.  There are about half a million transfers in our sample data set, far too many for manual analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read this data into your Spark shell with the following commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://c1ea9c6f2d55:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1525313190593)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.graphx._\n",
       "import scala.math._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.graphx._\n",
    "import scala.math._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Transfer\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Transfer(src:Long,dst:Long,amount:Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tx count: 500000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tx: org.apache.spark.rdd.RDD[Transfer] = MapPartitionsRDD[6] at map at <console>:40\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tx = sc.textFile(\"/home/jovyan/Resources/txfr.csv\")\n",
    "    .distinct\n",
    "    .map(_.split(\",\"))\n",
    "    .map(x => Transfer(x(0).toLong,x(1).toLong,x(2).toDouble))\n",
    "    .cache\n",
    "println(\"Tx count: \" + tx.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're importing stuff we'll need.  Next, we're defining a `Transfer` class to make interacting with our data a little easier.  Lastly, we're importing the data from the source file `tx.csv` in the lab directory, calling `distinct` on the source lines just to ensure we don't have any duplicates (our source system was imperfect), parsing the rows into `Transfer` objects, then caching the `RDD` in memory (and then printing the count of transactions just for good measure).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we're ready to graphify this data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify vertices\n",
    "\n",
    "The two things we'll need to identify in our source data are first, the vertices (also called \"nodes\"), and second, the edges, of our graph.  In this dataset, our vertices will consist of the accounts.  Before we can create the vertices, we need to get the distinct accounts that are present in the source data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accts: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[10] at distinct at <console>:39\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accts = tx.flatMap(x => Array(x.src,x.dst)).distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're simply `flatMap`ing an array of the source and destination accounts of each transfer, then removing duplicates with `distinct`.  Easy-peasy.  Now, let's turn these into vertices:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertices: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[11] at map at <console>:41\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertices = accts.map(x => (x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh?  Why are we mapping each account number to a tuple of the account number and itself?  The answer is pretty simple:  each vertex is identifed by a `VertexId` (which is a type alias for a `Long`), and the vertex's property type `VD`.  In this case, we're going to use the account number for both the `VertexId` and its property type, `VD`.  That means the type of `vertices` is then `RDD[(Long,Long)]`:  an `RDD` of a tuple of `Long` & `Long`.  Since `Long` is type-aliased to `VertexId`, `vertices` also can be thought of as `RDD[VertexId,Long]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set `vertices` aside for a moment and move on to the edges of our graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify edges\n",
    "\n",
    "Just as a vertex has a property type, so does an `Edge`.  `Edge`s connect two vertices together, so they naturally have a source vertex & and destination vertex, however, instead of holding references to actual vertices, the `Edge` holds the source & destination `VertexId`.  `Edge`s also have a property, denoted as `ED`.  Naturally, then, an `Edge`, which is a `case class`, has a constructor of type `Edge[ED](src: VertexId, dst: VertexId, property: ED)`.  Let's use this to build edges from our transaction data, `tx`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = MapPartitionsRDD[12] at map at <console>:39\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges = tx.map(x => Edge(x.src, x.dst, x.amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data, then, each edge is of type `Edge[Double]` (remember that `Transaction.src` & `Transaction.dst` are both `Long`, which is the same as `VertexId`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined both our vertices and our edges, let's define the graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the graph\n",
    "\n",
    "Spark GraphX's type for a graph is `org.apache.spark.graphx.Graph[VD, ED]`, where `VD` is the property type of its vertices and `ED` is the property type of its edges.  To construct one, simply use the `Graph` companion object's `apply` method, which, in its simplest form, is `apply[VD, ED](vertices: RDD[(VertexId, VD)], edges: RDD[Edge[ED]])`.  Let's do that now with our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@a189484\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = Graph(vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So simple is almost anticlimactic, right?  Now that we've got our graph instantiated, let's take a moment to visualize a small subset of it (see file `viz.scala` in this lab's solution directory if you're curious as to how this image was created):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transfer Data](../Resources/img/unbiased-subgraph-threshold-990.00.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's pretty random, which makes sense, since a random number generator was used to produce it.  Even for a human, it's pretty much impossible to identify anything that would lead us to answers to our BI department's questions.  What a mess, much like real life!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query over the graph\n",
    "\n",
    "It turns out we can answer the first three BI questions by querying over the graph.  The first question, which is basically \"Who is receiving the most transfers?\", can be answered by counting up all the `Edge`s pointing to each vertex â€” this is also called the \"degrees\" of the vertex, and there are two kinds:  inward pointing `Edge`s (from other vertices to \"this\" one), and outward pointing `Edge`s (from \"this\" one to other vertices).  GraphX provides us with three methods for these:  `degrees`, `inDegrees`, and `outDegrees`, respectively.  Let's use them to answer the three questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 receivers:\n",
      "(6548,93)\n",
      "(3262,84)\n",
      "(6668,84)\n",
      "(1819,83)\n",
      "(1545,82)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "top5in: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((6548,93), (3262,84), (6668,84), (1819,83), (1545,82))\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val top5in = graph.inDegrees.sortBy(_._2, ascending = false).take(5)\n",
    "println(\"Top 5 receivers:\")\n",
    "top5in.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `inDegrees` is returning to us a `VertexRDD[Int]`, which is the same as `RDD[(VertexId,Int)]`.  This means that we can simply sort in `inDegrees` by the number of inbound edges in descending order, then take whatever we want (5 in this case).  You should see the following results in your console:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "Top 5 receivers:\n",
    "(6548,93)\n",
    "(3262,84)\n",
    "(6668,84)\n",
    "(1819,83)\n",
    "(1545,82)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, plug & chug now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 senders:\n",
      "(9374,96)\n",
      "(2347,89)\n",
      "(7599,84)\n",
      "(7720,82)\n",
      "(3818,82)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "top5out: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((9374,96), (2347,89), (7599,84), (7720,82), (3818,82))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val top5out = graph.outDegrees.sortBy(_._2, ascending = false).take(5)\n",
    "println(\"Top 5 senders:\")\n",
    "top5out.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our top 5 accounts with outbound transfers are:\n",
    "\n",
    "``` \n",
    "Top 5 senders:\n",
    "(9374,96)\n",
    "(2347,89)\n",
    "(7599,84)\n",
    "(7720,82)\n",
    "(3818,82)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the most active users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most active:\n",
      "(9374,158)\n",
      "(2260,154)\n",
      "(2347,152)\n",
      "(8056,147)\n",
      "(2293,147)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "top5inout: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((9374,158), (2260,154), (2347,152), (8056,147), (2293,147))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val top5inout = graph.degrees.sortBy(_._2, false).take(5)\n",
    "println(\"Top 5 most active:\")\n",
    "top5inout.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "\n",
    "``` \n",
    "Top 5 most active:\n",
    "(9374,158)\n",
    "(2260,154)\n",
    "(2347,152)\n",
    "(8056,147)\n",
    "(2293,147)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we were able to do simply graph queries to find out some useful information for our BI folks, but what about the million dollar question?  BI wants to know who are the *most important* users of our platform.  How can we answer that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PageRank\n",
    "\n",
    "If you read section titles, which are hard to ignore, you might have guessed it already:  the PageRank algorithm, popularized (and monetized *ad nauseum*) by Google.  This algorithm attempts to identify which vertices in a graph are \"important\" by looking at which ones are pointed to the most, and which vertices those important ones point to.  If an important node points to another node, then that node's importance goes up, and so on.  Let's use it now to find out who our most important users are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 by PageRank:\n",
      "(6548,1.5284580685839666)\n",
      "(1819,1.4400866217180495)\n",
      "(8267,1.4243526719687647)\n",
      "(6668,1.423445151358255)\n",
      "(3262,1.420905481086376)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ranks: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[872] at RDD at VertexRDD.scala:57\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ranks = graph.pageRank(0.0001).vertices\n",
    "println(\"Top 5 by PageRank:\")\n",
    "ranks.sortBy(_._2, ascending = false).take(5).foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're called `pageRank` and giving it a threshold of 0.0001, meaning once changes in scores reach less than 0.0001, the algorithm declares itself finished.  We then take the vertices from the resulting `Graph[Double,Double]`, which represent the rankings of the vertices.  From there, it's a simple descending sort on the scores (the second element in the tuple) and take however many we want:\n",
    "\n",
    "``` \n",
    "Top 5 by PageRank:\n",
    "(6548,1.5275903676850071)\n",
    "(1819,1.4392690890150412)\n",
    "(8267,1.423544071380168)\n",
    "(6668,1.4226370659662904)\n",
    "(3262,1.4200988374573378)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there you have it!  Our top 5 *most important* accounts, as determined by PageRank!  Now, you have no choice but to believe it on faith, since we don't know this transfer data at all.  What if we could bias our data to see if our artificial anomalies produce data that we know we can believe (or at least lets us know that we're not crazy)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias the data\n",
    "\n",
    "Well, we've done that for you!  We added 50,000 transactions that are *intentionally* biased, so that accounts `1111`, `2222` & `3333` receive many more transfers than any of the others.  This data is contained in lab file `txfr-with-bias.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize (a small subset of) the data, to prove to ourselves that it's biased:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Biased Data](../Resources/img/biased-subgraph-threshold-990.00.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the three clumps in the middle?  Here, let me show you:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Annotated biased data](../Resources/img/biased-subgraph-threshold-990.00.annotated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrows above indicate the rough location of accounts 1111, 2222 & 3333 in our biased graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, repeat the steps above with that file, or just execute the following commands in your shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tx2 count: 550000\n",
      "Top 5 receivers:\n",
      "(3333,11345)\n",
      "(2222,11288)\n",
      "(1111,11266)\n",
      "(6548,83)\n",
      "(6668,82)\n",
      "Top 5 senders:\n",
      "(9374,93)\n",
      "(2830,84)\n",
      "(7282,83)\n",
      "(3818,82)\n",
      "(8056,82)\n",
      "Top 5 most active:\n",
      "(3333,11390)\n",
      "(2222,11341)\n",
      "(1111,11317)\n",
      "(9374,152)\n",
      "(8056,145)\n",
      "2018-05-03 02:09:37 WARN  BlockManager:66 - Block rdd_937_1 already exists on this machine; not re-adding it\n",
      "Top 5 by PageRank:\n",
      "(1111,182.0755551990941)\n",
      "(2222,175.6275108494563)\n",
      "(3333,166.5451150308563)\n",
      "(6548,4.342198327244957)\n",
      "(1048,4.219686484992292)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tx2: org.apache.spark.rdd.RDD[Transfer] = MapPartitionsRDD[890] at map at <console>:37\n",
       "accts2: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[894] at distinct at <console>:39\n",
       "vertices2: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[895] at map at <console>:40\n",
       "edges2: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = MapPartitionsRDD[897] at map at <console>:41\n",
       "graph2: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@1f99cc81\n",
       "top5in2: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((3333,11345), (2222,11288), (1111,11266), (6548,83), (6668,82))\n",
       "top5out2: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((9374,93), (2830,84), (7282,83), (3818,82), (8056,82))\n",
       "top5inout2: Array[(org.apache.spark.graphx.VertexId, In..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tx2 = sc.textFile(\"/home/jovyan/Resources/txfr-with-bias.csv\").distinct.map(_.split(\",\")).map(x => Transfer(x(0).toLong,x(1).toLong,x(2).toDouble)).cache\n",
    "println(\"Tx2 count: \" + tx2.count)\n",
    "val accts2 = tx2.flatMap(x => Array(x.src,x.dst)).distinct\n",
    "val vertices2 = accts2.map(x => (x, x))\n",
    "val edges2 = tx2.filter(_.amount > 100).map(x => Edge(x.src, x.dst, x.amount))\n",
    "\n",
    "val graph2 = Graph(vertices2, edges2)\n",
    "\n",
    "val top5in2 = graph2.inDegrees.sortBy(_._2, false).take(5)\n",
    "println(\"Top 5 receivers:\")\n",
    "top5in2.foreach(println(_))\n",
    "\n",
    "val top5out2 = graph2.outDegrees.sortBy(_._2, false).take(5)\n",
    "println(\"Top 5 senders:\")\n",
    "top5out2.foreach(println(_))\n",
    "\n",
    "val top5inout2 = graph2.degrees.sortBy(_._2, false).take(5)\n",
    "println(\"Top 5 most active:\")\n",
    "top5inout2.foreach(println(_))\n",
    "\n",
    "val ranks2 = graph2.pageRank(0.0001).vertices\n",
    "println(\"Top 5 by PageRank:\")\n",
    "ranks2.sortBy(_._2, ascending = false).take(5).foreach(println(_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like the following:\n",
    "\n",
    "``` \n",
    "...\n",
    "Top 5 receivers:\n",
    "(3333,11345)\n",
    "(2222,11288)\n",
    "(1111,11266)\n",
    "(6548,83)\n",
    "(6668,82)\n",
    "...\n",
    "Top 5 senders:\n",
    "(9374,93)\n",
    "(2830,84)\n",
    "(7282,83)\n",
    "(3818,82)\n",
    "(8056,82)\n",
    "...\n",
    "Top 5 most active:\n",
    "(3333,11390)\n",
    "(2222,11341)\n",
    "(1111,11317)\n",
    "(9374,152)\n",
    "(8056,145)\n",
    "...\n",
    "Top 5 by PageRank:\n",
    "(1111,181.9586873478817)\n",
    "(2222,175.5147817695734)\n",
    "(3333,166.43821561922303)\n",
    "(6548,4.339411223904872)\n",
    "(1048,4.216978017665352)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense.  Since we've biased our data toward transfers into accounts 1111, 2222 & 3333, it makes sense that their importance goes up, because importance is determined, according to PageRank, in part by how many vertices point toward another vertex.  The more vertices that point toward a vertex, the more important that vertex is scored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We saw in this lab how we can create graphs of data and perform fairly simply calculations over them to answer questions that could have great business value.  We also saw how we can use more sophisticated graph analytics like PageRank to glean even more information from a graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
