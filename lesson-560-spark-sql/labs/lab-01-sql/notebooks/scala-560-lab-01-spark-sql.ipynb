{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will take your first steps using Spark's support for SQL and relational concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "1. Use the `SqlContext` to query data as relational tables using different APIs.\n",
    "2. Create a user-defined SQL function and use it in a SQL query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a SQLContext\n",
    "\n",
    "In order to work with Spark's SQL support, we need to first get our hands on a special context called `SQLContext`.  First, let's start with some appropriate imports.  Issue the following commands in the Spark shell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util._\n",
    "import org.apache.spark.sql._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get a `SQLContext`.  Some Spark shells include one by default; to handle these cases, issue the following command. So if you decide to run it in a SparkShell, you may want to run this instead.\n",
    "\n",
    "```scala\n",
    "val ctx: SQLContext = Try(sqlContext).getOrElse(new SQLContext(sc))\n",
    "```\n",
    "\n",
    "However, in Notebooks you should be able to get away with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ctx: SQLContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensures that we've got a `SQLContext` called `ctx` whether or not one was defined.  Next, we want to make sure that we're getting all the nice Spark SQL implicits that the `SQLContext` provides.  Issue the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctx.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to roll with Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a case class to represent your data\n",
    "\n",
    "One of the easiest ways to get going with Spark SQL is to define the schema of your data via a case class.  Since we're using our sample transaction data, we'll define a simple case class called `Tx`.  Enter the following command into your shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Tx(date: String, desc: String, amount: Double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note:  To use Spark SQL effectively, your data must be structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some data\n",
    "\n",
    "We'll use some data provide. Let's get some data to work with by creating a simple `RDD` from a sample transaction data file called `tx.csv` found in this course's `lesson-5xx-resources` directory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc.textFile(\"/home/jovyan/Resources/tx.csv\").map(_.split(\",\")).map(x => Tx(x(0), x(1), x(2).toDouble)).cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we added the `cache` method call to keep the `RDD` in memory once it's loaded; this will improve performance as we query the data in subsequent steps during this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a `DataFrame`\n",
    "\n",
    "The primary, table-like artifact in Spark SQL that represents data is called a `DataFrame`.  Thanks to some clever implicits defined by `ctx.implicits`, there's a new method on `RDD` called `toDF()` which returns a `DataFrame` with its dataset being that of the `RDD` from whence the `DataFrame` came.\n",
    "\n",
    "> Note:  *Any* `RDD` can serve as the basis for a `DataFrame`.\n",
    "\n",
    "A `DataFrame` can essentially be thought of as a table, with columns and rows.  Often, the schema of the table, that is, the names and types of the columns, can be inferred, but it can also be programmatically defined.\n",
    "\n",
    "Let's get a `DataFrame` of our transaction data now.  Issue the command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val table = rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in your shell.  This returns to us a `DataFrame` whose schema has been inferred from our `Tx` case class.  Remember, the original `RDD` was of type `RDD[Tx]`, so the `DataFrame` can inspect the `Tx` class for the names & types of its fields and assume that these define corresponding columns.\n",
    "\n",
    "Take a look at the inferred schema by issuing command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output like the following:\n",
    "\n",
    "``` shell\n",
    "root\n",
    " |-- date: string (nullable = true)\n",
    " |-- desc: string (nullable = true)\n",
    " |-- amount: double (nullable = false)\n",
    "```\n",
    "\n",
    "As you can see, these columns are derived from our case class `Tx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the `DataFrame` as a table\n",
    "\n",
    "Now, let's give our `DataFrame` a name so that we can query it via SQL.  We do this by \"registering\" a name for the `DataFrame` with the `SQLContext`.  Issue the following command in your shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.registerTempTable(\"tx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will cause the `DataFrame` to register itself in its `SQLContext` with the given name, in this case, `tx`.  That's all it takes in order for us to be able to query data in the Spark cluster via SQL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query some data with SQL\n",
    "\n",
    "Let's do something simple to start out with:  count the number of rows in the table.  Issue the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.sql(\"SELECT COUNT(*) FROM tx\").collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should simply be\n",
    "\n",
    "```\n",
    "[1265]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  You should start realizing the possibilities here.  Any program that speaks JDBC (or ODBC) can connect to a Spark cluster and start querying data, including COTS business intelligence tools (BIRT, etc) and custom programs.\n",
    "\n",
    "Let's sink our teeth into Spark SQL's juicy SQL flesh by finding the top 10 credits in our transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.sql(\"SELECT * FROM tx WHERE amount > 0 ORDER BY amount DESC LIMIT 10\") .collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be the following:\n",
    "\n",
    "```\n",
    "[2015-12-15,Deposit - Online Banking Transfer from XXXXXXXXXX SAV,2209.03]\n",
    "[2015-08-05,Deposit - Shared Branch 5011 W SLAUGHTER LN BLD AUSTIN        TX,1520.0]\n",
    "[2015-12-15,Deposit - Online Banking Transfer from XXXXXXXXXX CK,1360.48]\n",
    "[2015-12-07,Deposit - Online Banking Transfer from XXXXXXXXXX CK,1000.0]\n",
    "[2015-12-11,Deposit - Online Banking Transfer from XXXXXXXXXX CK,925.0]\n",
    "[2015-12-04,Deposit - Online Banking Transfer from XXXXXXXXXX CK,800.0]\n",
    "[2015-06-19,Deposit - Online Banking Transfer from XXXXXXXXXX SAV,750.31]\n",
    "[2015-07-26,Deposit - Online Banking Transfer from XXXXXXXXXX CK,650.0]\n",
    "[2015-07-10,Deposit - Online Banking Transfer from XXXXXXXXXX SAV,500.0]\n",
    "[2015-07-07,Deposit - Online Banking Transfer from XXXXXXXXXX SAV,500.0]\n",
    "```\n",
    "\n",
    "It's a Thing of Beauty!  This data is stored in who-knows-what format in who-knows-what-kind-of-distributed-cluster, but we can query it like it's a table!  Let's keep going.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query some data with Spark SQL's query DSL\n",
    "\n",
    "For those of you who groan at the sight of strings carrying program logic, this might just be for you.  Spark SQL includes a simple but effective query DSL that can mitigate errors in otherwise stringy SQL that can't be checked at compile-time.  Let's perform the same query, only this time, let's use the query DSL.  Issue the following command in your shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.filter(\"amount > 0\") .orderBy(table.col(\"amount\").desc).limit(10).collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've greatly reduced the portion of our code that is contained in strings that are opaque to the compiler!\n",
    "\n",
    "> Note:  Spark 1.6 will take type safety even farther with a new artifact called a `DataSet`, which allows your query DSL-based code to be *completely* type-safe.\n",
    "\n",
    "Your output should be identical to your earlier query.  Let's keep going with the complementary query.  What are the biggest debits in our transaction data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.sql(\"SELECT * FROM tx WHERE amount < 0 ORDER BY amount LIMIT 10\") .collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be the following:\n",
    "\n",
    "```\n",
    "[2015-12-14,External Withdrawal - PAYPAL INSTANT TRANSFER - INST XFER,-925.0]\n",
    "[2015-12-07,ATM Withdrawal - CAPITAL ONE 7200 NORTH MOPAC       AUSTIN       TXUS - Card Ending In 2000,-803.0]\n",
    "[2015-12-04,ATM Withdrawal - BECU 35105 ENCHANTED PARKWAYFEDERAL WAY  WAUS - Card Ending In 7090,-800.0]\n",
    "[2015-06-18,POS Withdrawal - 1921254 DISCOUNT  DISCOUNT TIR AUSTIN       TXUS - Card Ending In 7090,-750.31]\n",
    "[2015-06-17,Withdrawal - Online Banking Transfer To XXXXXXXXXX CK,-511.36]\n",
    "[2015-07-28,External Withdrawal - PAYPAL INSTANT TRANSFER - INST XFER,-408.71]\n",
    "[2015-06-27,ATM Withdrawal - ULTRON PROCESSI HARRAH'S NEW ORLEANS   NEW ORLEANS  LAUS - Card Ending In 2000,-405.99]\n",
    "[2015-06-18,External Withdrawal - FCU  -  Bac,-350.0]\n",
    "[2015-07-06,Withdrawal - Shared Branch 5029 KYLE CENTER DR     KYLE          TX,-350.0]\n",
    "[2015-08-14,POS Withdrawal - 879257 HEB #611               DRIPPING SPRITXUS - Card Ending In 7090,-335.12]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, via the query DSL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.filter(\"amount < 0\") .orderBy(table.col(\"amount\")).limit(10).collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the results should be identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend SQL with your own functions\n",
    "\n",
    "Many SQL databases allow you to define your own functions and call them in your queries; these are called \"user-defined functions\", or UDFs.  Spark also supports these, and you can write them in Scala, Java, Python, or R.  Let's check it out.\n",
    "\n",
    "Our custom function will be very simple:  it calculates the trimmed length of a string.  If you've ever done this in pure SQL, you've felt The Pain.  However, it's trivial in Scala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val strlen = (s: String) => s.trim.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter that command in your shell, then register it as a UDF in your `SQLContext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.udf.register(\"len\", strlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've registered our function `strlen` in the `SQLContext` with the name `len`.  Now, let's use it to find extremely long transaction descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.sql(\"SELECT len(desc), desc FROM tx WHERE len(desc) >= 100 ORDER BY len(desc) DESC\") .collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are our results:\n",
    "\n",
    "```\n",
    "[107,External Deposit - AMAZON.COM ID3LXQ2E - Marketplac  payments.amazon.com ID#U6X42QZ19K9LLOW U6X42QZ19K9LLOW]\n",
    "[107,External Deposit - AMAZON.COM IESC1WFF - Marketplac  payments.amazon.com ID#QNQK9Z43WLNNVIB QNQK9Z43WLNNVIB]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two transactions whose length exceeds 100 characters.  Notice that we've used our UDF in the `SELECT` clause, the `WHERE` clause, and the `ORDER BY` clause!  Nice, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables from other types of data\n",
    "\n",
    "Spark SQL supports directly creating `DataFrame`s from several different formats natively, including JSON, Parquet, and even JDBC.  That's right:  you can create your own tables in the cluster from data in some *external* JDBC data source!  Let's have a look at reading some JSON data.\n",
    "\n",
    "Let's read the sample transaction data that's been provided in JSON format.  Here's a snippet of what it looks like:\n",
    "\n",
    "``` json\n",
    "{ \"date\": \"2015-06-16\", \"desc\": \"POS Withdrawal - 75901 CORNER STORE 13        DRIPPING SPRITXUS - Card Ending In 7090\", \"amount\":-66.21 }\n",
    "{ \"date\": \"2015-06-16\", \"desc\": \"Withdrawal - Online Banking Transfer To XXXXXXXXXX CK\", \"amount\":-50.0 }\n",
    "{ \"date\": \"2015-06-16\", \"desc\": \"POS Withdrawal - 879042 HEB GAS #404           AUSTIN       TXUS - Card Ending In 7090\", \"amount\":-45.49 }\n",
    "```\n",
    "\n",
    "> Warning:  There's one thing to note about Spark SQL's JSON support:  each line must be a complete JSON object.  That's not usually the case with JSON.  Most blocks of JSON data are multiline.  The data above may be wrapping due to current margins, but each JSON object above is actually on one line.\n",
    "\n",
    "To read the JSON data, issue the following commands in your shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val jt = ctx.read.json(\"/home/jovyan/Resources/tx.jsons\")\n",
    "jt.registerTempTable(\"jtx\")\n",
    "jt.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like the following:\n",
    "```\n",
    "jt: org.apache.spark.sql.DataFrame = [amount: double, date: string, desc: string]\n",
    "\n",
    "root\n",
    " |-- amount: double (nullable = true)\n",
    " |-- date: string (nullable = true)\n",
    " |-- desc: string (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, where did the `DataFrame` get that schema from?  Well, since the source is JSON, there is type information implicitly available!  In JSON, everything is a string that's wrapped in double-quotes.  Anything else has a definite type that can be inferred!  Here, we see that `amount` has been inferred to be a number, so the `DataFrame` is using `double`.\n",
    "\n",
    "Now, let's issue the same query as before:  what are the biggest credits in the transaction data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.sql(\"SELECT len(desc), desc FROM jtx WHERE len(desc) >= 100 ORDER BY len(desc) DESC\") .collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "``` scala\n",
    "[2209.03,2015-12-15,Deposit - Online Banking Transfer from XXXXXXXXXX SAV]\n",
    "[1520.0,2015-08-05,Deposit - Shared Branch 5011 W SLAUGHTER LN BLD AUSTIN        TX]\n",
    "[1360.48,2015-12-15,Deposit - Online Banking Transfer from XXXXXXXXXX CK]\n",
    "[1000.0,2015-12-07,Deposit - Online Banking Transfer from XXXXXXXXXX CK]\n",
    "[925.0,2015-12-11,Deposit - Online Banking Transfer from XXXXXXXXXX CK]\n",
    "[800.0,2015-12-04,Deposit - Online Banking Transfer from XXXXXXXXXX CK]\n",
    "[750.31,2015-06-19,Deposit - Online Banking Transfer from XXXXXXXXXX SAV]\n",
    "[650.0,2015-07-26,Deposit - Online Banking Transfer from XXXXXXXXXX CK]\n",
    "[500.0,2015-07-07,Deposit - Online Banking Transfer from XXXXXXXXXX SAV]\n",
    "[500.0,2015-07-10,Deposit - Online Banking Transfer from XXXXXXXXXX SAV]\n",
    "```\n",
    "\n",
    "Notice that this data is the same data as we got before.  The only difference is the column order.  Pretty cool, n'est-ce pas?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you've seen how you can use conventional SQL to query data stored in a Spark cluster, and that you can import data from many different sources, including other SQL data sources.  This is just the tip of the iceberg:  you can not only query data, but you can modify it, too, or even store `DataFrames` as permanent SQL tables in your cluster.  Further, Spark SQL even supports Hive & HiveQL via, you guessed it, `HiveContext`!  The sky's the limit now!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "#### Find the amount totals for every month\n",
    "\n",
    "#### Find the account balance at the end of each month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using plain SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.sum\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"Scala SQL Challenge\").getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "case class Tx(date: String, desc: String, amount: Double)\n",
    "val txn_rdd = spark.sparkContext.textFile(\"/home/jovyan/Resources/tx.csv\").map(_.split(\",\")).map(x => Tx(x(0), x(1), x(2).toDouble)).cache\n",
    "\n",
    "val txn_df = txn_rdd.toDF\n",
    "txn_df.createOrReplaceTempView(\"txn_df\")\n",
    "\n",
    "val txn_month = spark.sql(\"select substr(date, 1, 7) as month, sum(amount) as amnount from txn_df group by substr(date, 1, 7) order by substr(date, 1, 7)\")\n",
    "txn_month.show\n",
    "\n",
    "var tot = 0.0\n",
    "for (mon <- txn_month.collect) {\n",
    "  tot = tot + mon.getDouble(1)\n",
    "  println(mon.getString(0) + \"\\t\" + tot.toString)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.sum\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"Scala SQL Challenge\").getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "case class Tx(date: String, desc: String, amount: Double)\n",
    "val txn_rdd = spark.sparkContext.textFile(\"/home/jovyan/Resources/tx.csv\").map(_.split(\",\")).map(x => Tx(x(0), x(1), x(2).toDouble)).cache\n",
    "\n",
    "val txn_df = txn_rdd.toDF\n",
    "\n",
    "val txn_month = txn_df.select(txn_df(\"date\").substr(1, 7).alias(\"month\"), txn_df(\"amount\"))\n",
    "\n",
    "val txn_group = txn_month.groupBy(\"month\").agg(sum(\"amount\").alias(\"amount\")).orderBy(\"month\")\n",
    "\n",
    "txn_group.show\n",
    "\n",
    "var tot = 0.0\n",
    "for (mon <- txn_group.collect) {\n",
    "  tot = tot + mon.getDouble(1)\n",
    "  println(mon.getString(0) + \"\\t\" + tot.toString)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
