{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will take your first steps using Spark's support for SQL and relational concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "1. Use the `SparkSession` to query data as relational tables using different APIs.\n",
    "2. Create a user-defined SQL function and use it in a SQL query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a SparkSession\n",
    "\n",
    "In order to work with Spark's SQL support, we need to first get our hands on a special context called `SparkSession`.  \n",
    "The SparkSession class is the entry point into all functionality in Spark. \n",
    "\n",
    "> Note: as of Spark 2.0, SparkSession replaced SqlContext. However, we could still use SqlContext as it's being kept for backward compatibility.\n",
    "\n",
    "We'll use SparkSession.builder to create a SparkSession. SparkSession.builder lets you define you application name and it also lets you set various parameters in the Spark config, although there is no need to do so for our simple example.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to roll with Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Get some data\n",
    "\n",
    "Let's get some data to work with by creating a simple RDD from a sample transaction data file called tx.csv found in this course's data directory.\n",
    "\n",
    "Issue the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.sparkContext.textFile(\"/home/jovyan/Resources/tx.csv\")\n",
    "\n",
    "print(lines.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map to transactions\n",
    "\n",
    "The individual data items in the lines of text are comma separated, thefore we'll be using split function to parse the lines into individual items. Parsed items will be mapped into tuples representing transactions. Note, we're converting the amount into `float` to allow for numeric operations.\n",
    "\n",
    "Let's take a sample of transactions to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txns = lines.map(lambda st: st.split(\",\")).map(lambda el: (el[0], el[1], float(el[2])))\n",
    "\n",
    "txns.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a `DataFrame`\n",
    "\n",
    "The primary, table-like artifact in Spark SQL that represents data is called a `DataFrame`.  We'll be using an `RDD`s function called `toDF()` to transform it into a `DataFrame`.\n",
    "\n",
    "> Note:  *Any* `RDD` can serve as the basis for a `DataFrame`.\n",
    "\n",
    "A `DataFrame` can essentially be thought of as a table, with columns and rows.  Often, the schema of the table, that is, the names and types of the columns, can be inferred; this will happen when RDD uses elements of defined types. In our case the `RDD` transactions are represented by simple tuples so only the data types are inferred. Luckily we can define the column names by simply passing them to `toDF` function as an array. \n",
    "\n",
    "Let's get a `DataFrame` of our transaction data now.  Issue the command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = txns.toDF([\"date\", \"desc\", \"amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the DataFrame's schema by issuing command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also take a peek at the content of `DataFrame` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL Queries Programmatically\n",
    "\n",
    "`DataFrame`s can be queried using SQL but they need to be registered as temporary views first.\n",
    "\n",
    "Registering is done by `DataFrame`s `createOrReplaceTempView` function, which accepts a parameter with the view name.\n",
    "\n",
    "SQL query is executed via `SparkSession.sql` function are it returns the result as another `DataFrame`.\n",
    "\n",
    "That's all it takes in order for us to be able to query data in the Spark cluster via SQL!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"txns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query some data with SQL\n",
    "\n",
    "Let's do something simple to start out with:  count the number of rows in the table.  Issue the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) AS count FROM txns\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  You should start realizing the possibilities here.  Any program that speaks JDBC (or ODBC) can connect to a Spark cluster and start querying data, including COTS business intelligence tools (BIRT, etc) and custom programs.\n",
    "\n",
    "Let's sink our teeth into Spark SQL's juicy SQL flesh by finding the top 10 credits in our transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM txns WHERE amount > 0 ORDER BY amount DESC LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a Thing of Beauty!  This data is stored in who-knows-what format in who-knows-what-kind-of-distributed-cluster, but we can query it like it's a table!  Let's keep going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query some data with Spark SQL's query DSL\n",
    "\n",
    "For those of you who groan at the sight of strings carrying program logic, this might just be for you.  Spark SQL includes a simple but effective query DSL that can mitigate errors in otherwise stringy SQL that can't be checked at compile-time.  Let's perform the same query, only this time, let's use the query DSL.  Issue the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.filter(df[\"amount\"] > 0).orderBy(\"amount\", ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've greatly reduced the portion of our code that is contained in strings that are opaque to the compiler!\n",
    "\n",
    "> Note:  Spark 1.6 will take type safety even farther with a new artifact called a `DataSet`, which allows your query DSL-based code to be *completely* type-safe.\n",
    "\n",
    "Your output should be identical to your earlier query.  Let's keep going with the complementary query.  What are the biggest debits in our transaction data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM txns WHERE amount < 0 ORDER BY amount LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, via the query DSL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.filter(df[\"amount\"] < 0).orderBy(\"amount\").limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the results should be identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend SQL with your own functions\n",
    "\n",
    "Many SQL databases allow you to define your own functions and call them in your queries; these are called \"user-defined functions\", or UDFs.  Spark also supports these, and you can write them in Scala, Java, Python, or R.  Let's check it out.\n",
    "\n",
    "Our custom function will be very simple:  it calculates the trimmed length of a string.  If you've ever done this in pure SQL, you've felt The Pain.  However, it's trivial in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strlen(s):\n",
    "  return len(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to register it as a UDF in your `SparkSesssion`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.udf.register(\"len\", strlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've registered our function `strlen` in the `SparkSesssion` with the name `len`.  Now, let's use it to find extremely long transaction descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT len(desc), desc FROM txns WHERE len(desc) >= 100 ORDER BY len(desc) DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know the original descriptions use collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT len(desc), desc FROM txns WHERE len(desc) >= 100 ORDER BY len(desc) DESC\").collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two transactions whose length exceeds 100 characters.  Notice that we've used our UDF in the `SELECT` clause, the `WHERE` clause, and the `ORDER BY` clause!  Nice, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables from other types of data\n",
    "\n",
    "Spark SQL supports directly creating `DataFrame`s from several different formats natively, including JSON, Parquet, and even JDBC.  That's right:  you can create your own tables in the cluster from data in some *external* JDBC data source!  Let's have a look at reading some JSON data.\n",
    "\n",
    "Let's read the sample transaction data that's been provided in JSON format.  Here's a snippet of what it looks like:\n",
    "\n",
    "``` json\n",
    "{ \"date\": \"2015-06-16\", \"desc\": \"POS Withdrawal - 75901 CORNER STORE 13        DRIPPING SPRITXUS - Card Ending In 7090\", \"amount\":-66.21 }\n",
    "{ \"date\": \"2015-06-16\", \"desc\": \"Withdrawal - Online Banking Transfer To XXXXXXXXXX CK\", \"amount\":-50.0 }\n",
    "{ \"date\": \"2015-06-16\", \"desc\": \"POS Withdrawal - 879042 HEB GAS #404           AUSTIN       TXUS - Card Ending In 7090\", \"amount\":-45.49 }\n",
    "```\n",
    "\n",
    "> Warning:  There's one thing to note about Spark SQL's JSON support:  each line must be a complete JSON object.  That's not usually the case with JSON.  Most blocks of JSON data are multiline.  The data above may be wrapping due to current margins, but each JSON object above is actually on one line.\n",
    "\n",
    "To read the JSON data, issue the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jt = spark.read.json(\"/home/jovyan/Resources/tx.jsons\")\n",
    "\n",
    "jt.createOrReplaceTempView(\"jtx\")\n",
    "\n",
    "jt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, where did the `DataFrame` get that schema from?  Well, since the source is JSON, there is type information implicitly available!  In JSON, everything is a string that's wrapped in double-quotes.  Anything else has a definite type that can be inferred!  Here, we see that `amount` has been inferred to be a number, so the `DataFrame` is using `double`.\n",
    "\n",
    "Now, let's issue the same query as before:  what are the biggest credits in the transaction data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM jtx WHERE amount > 0 ORDER BY amount DESC LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this data is the same data as we got before.  The only difference is the column order.  Pretty cool, n'est-ce pas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you've seen how you can use conventional SQL to query data stored in a Spark cluster, and that you can import data from many different sources, including other SQL data sources.  This is just the tip of the iceberg:  you can not only query data, but you can modify it, too, or even store `DataFrames` as permanent SQL tables in your cluster.  Further, Spark SQL even supports Hive & HiveQL via, you guessed it, `HiveContext`!  The sky's the limit now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "#### Find the amount totals for every month\n",
    "\n",
    "#### Find the account balance at the end of each month\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "txn_month = df.select(df[\"date\"][1:7].alias(\"month\"), df[\"amount\"])\n",
    "\n",
    "# txn_month.createOrReplaceTempView(\"txn_month\")\n",
    "# txn_group = spark.sql(\"select month, sum(amount) as amount from txn_month group by month order by month\")\n",
    "\n",
    "txn_group = txn_month.groupBy(\"month\").agg(func.sum(\"amount\").alias(\"amount\")).orderBy(\"month\")\n",
    "txn_group.show()\n",
    "\n",
    "tot = 0\n",
    "for mon in txn_group.collect():\n",
    "   tot = tot + mon[1]\n",
    "   print(mon[0], round(tot, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
