{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Python Pure with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "\n",
    "The main reason for this notebook is to illustrate the differences in set operations between Spark RDD and Python Collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### Configure the Context\n",
    "\n",
    "The code below should be run once and only once to setup the SparcContext object as `sc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me work with the naming convension `s*` for Spark RDD objects and `'p*` for Python Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a few sets in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pInitialSet = {1,2,3}\n",
    "pSecondSet = {4, 5}\n",
    "pThirdSet = {2,3,4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same using RDD's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sInitialSet = sc.parallelize({1,2,3})\n",
    "sSecondSet = sc.parallelize({4,5})\n",
    "sThirdSet = sc.parallelize({2,3,4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union\n",
    "\n",
    "Both Python and RDD name the method `union`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pInitialSet.union(pSecondSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sInitialSet.union(sSecondSet).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection\n",
    "\n",
    "Again, Python and RDD uses the same name `intersection` for this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pInitialSet.intersection(pThirdSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sInitialSet.intersection(sThirdSet).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract\n",
    "\n",
    "Subtracting is difference. Python calls this `difference`; RDD uses the name `subtract`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pInitialSet.difference(pThirdSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sInitialSet.subtract(sThirdSet).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size, find min and max of collection\n",
    "\n",
    "Finding the size, the min or max of a collection is quite different from Python to RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(pInitialSet))\n",
    "print(min(pInitialSet))\n",
    "print(max(pInitialSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sInitialSet.count())\n",
    "print(sInitialSet.min())\n",
    "print(sInitialSet.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartesian Product\n",
    "\n",
    "Cartesian products are not directly supported by Python collections, but can be added by importing `itertools`. So in Pyton you first have to import itertools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then you can use the `itertoos.product`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(itertools.product(pInitialSet,pSecondSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD's support carteasian products directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sInitialSet.cartesian(sSecondSet).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate\n",
    "\n",
    "Aggregate is an importon monoid in RDD. \n",
    "To aggregate, we need to pass:\n",
    "\n",
    "* An inital value for the accumulator\n",
    "* A lambda for combining the accumulator and the value\n",
    "* A lambda for combining two accumulators\n",
    "\n",
    "E.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sInitialSet.aggregate(\n",
    "    0, # Initial value for the accumulator\n",
    "    lambda acc, val: acc + val, # how do you combine the accumulator with the value?\n",
    "    lambda acc1, acc2: acc1 + acc2 # How do you add two accumulators together?\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting RDD's\n",
    "\n",
    "RDD's can be persisted easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistedSet = sc.parallelize(range(1,1000)).cache()\n",
    "equivalentSet = sc.parallelize(range(1,1000)).persist() # Uses default storage level MEMORY_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify the storage level, you have to first import the StorageLevel type. \n",
    "\n",
    "> In Python, stored objects will always be serialized with the Pickle library, so it does not matter whether you choose a serialized level. The available storage levels in Python include MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, and DISK_ONLY_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anotherPersistedSet = sc.parallelize(range(1,1000)).persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Classes\n",
    "\n",
    "Scala has case classes that can be used as types for RDD's. \n",
    "Python does not have case classes. Instead, we can use nametupes. \n",
    "This approach, requires that we import nametuples from collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from dateutil.parser import parse # I'll need this for parsing dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use nametuples to create a type. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX = namedtuple(\"TX\", [\"date\", \"amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "someDate = TX(parse(\"2018-01-02\"), float(\"123.12\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, a = someDate\n",
    "print(d)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
