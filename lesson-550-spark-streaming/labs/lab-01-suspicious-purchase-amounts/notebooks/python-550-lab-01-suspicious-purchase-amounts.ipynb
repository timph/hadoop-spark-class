{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming with state using QueueStream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from random import random\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Streaming\").getOrCreate()\n",
    "\n",
    "\n",
    "# update functon for updating the state\n",
    "def updateFunc(new_values, last_state):\n",
    "\n",
    "    # handling the state that hasn't been created yet\n",
    "    state = (0, 0, 0) if (last_state is None)  else last_state\n",
    "    lst = list(state)\n",
    "    for new_value in new_values:\n",
    "      lst[0] = lst[0]+new_value[0]\n",
    "      lst[1] = lst[1]+new_value[1]\n",
    "    lst[2]=0 if (lst[0] == 0) else lst[1]/lst[0]\n",
    "\n",
    "    return tuple(lst)\n",
    "\n",
    "\n",
    "# batch interval duration in seconds\n",
    "intervalDuration = 2                   \n",
    "\n",
    "fraud_factor = 1.33\n",
    "\n",
    "min_amount = 0.01\n",
    "max_amount = 10.0\n",
    "suspicious_amount = 10000.0\n",
    "\n",
    "# creating a StreamingContext with the batch interval of interval seconds\n",
    "ssc = StreamingContext(spark.sparkContext, intervalDuration)\n",
    "\n",
    "# checkpoint for backups\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# create a RDD queue to feed the stream\n",
    "rddQueue = []\n",
    "for i in range(1, 6):\n",
    "    txns = []\n",
    "    for j in range(16):\n",
    "        txns += [(\"date\", \"description\", -random()*(max_amount-min_amount)+min_amount)]\n",
    "    txns += [(\"date\", \"description\", -i*suspicious_amount)]\n",
    "    rddQueue += [ssc.sparkContext.parallelize(txns)]\n",
    "\n",
    "# create the QueueInputDStream\n",
    "debitTxns = ssc.queueStream(rddQueue)\n",
    "\n",
    "# we need to add the key to transactions to be able to compare with the amount mean\n",
    "# in the real application it would be more natural to use the account id as the key\n",
    "keyedTxns = debitTxns.map(lambda s: (1, s))\n",
    "\n",
    "# getting transaction amounts and updating the state holding amount mean\n",
    "amounts = debitTxns.map(lambda s: (1, (1, s[2], s[2])))\n",
    "meanAmount = amounts.updateStateByKey(updateFunc)\n",
    "meanAmount.pprint()\n",
    "\n",
    "# joining two streams with the purchase transactions and the mean\n",
    "joinedTxns = keyedTxns.join(meanAmount)\n",
    "\n",
    "# getting suspicious purchases\n",
    "suspiciousTxns = joinedTxns.map(lambda v: v[1]).filter(lambda t: t[0][2] < t[1][2]*fraud_factor).map(lambda t: t[0])\n",
    "\n",
    "suspiciousTxns.pprint()\n",
    "\n",
    "ssc.start()             # starting the computation\n",
    "ssc.awaitTermination()  # waiting for the computation to terminate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
