{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suspicious Purchase Amounts with Stateful Streaming\n",
    "\n",
    "In this lab, you will see how to use Spark Streaming to identify suspicious activity from a stream of purchase data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Use Spark Streaming's support for maintaining state of data to watch an incoming stream of transactions to determine the presence of anomalies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream's State\n",
    "\n",
    "Spark Streaming allows for maintaining an arbitary state of the stream, which is continuously updated with the new information as it arrives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Anomalies\n",
    "\n",
    "We'll define a transaction anomaly as a transaction with debit exceeding the average debits by defined threshold since the streaming started.\n",
    "\n",
    "In order to determine the average we will maintain the state, which is defined by the tuple:\n",
    "\n",
    "```python\n",
    "(debitCount, debitTotal, debitAverage)\n",
    "```\n",
    "\n",
    "Why do we need three values as opposed to just the average?\n",
    "\n",
    "This is because in order to recalculate the average correctly we need to know how many debits and the debits total it was based on.\n",
    "\n",
    "The state will be updated by the following function:\n",
    "\n",
    "```python\n",
    "updateStateByKey(func)\n",
    "```\n",
    "\n",
    "This function return a new state of the stream (which is the stream itself) where the state for each key is updated by applying the given function operating on the previous state of the key and the new values for the key. \n",
    "\n",
    "We'll see how it works in a bit.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Streaming\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Streaming Context with Batch Interval Duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# the duration is in seconds\n",
    "intervalDuration = 2\n",
    "\n",
    "ssc = StreamingContext(spark.sparkContext, intervalDuration)\n",
    "\n",
    "# checkpoint for backups\n",
    "ssc.checkpoint(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define \"update state\" Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunc(new_values, last_state):\n",
    "\n",
    "    # handling the state that hasn't been created yet\n",
    "    state = (0, 0, 0) if (last_state is None)  else last_state\n",
    "    lst = list(state)\n",
    "    for new_value in new_values:\n",
    "      lst[0] = lst[0]+new_value[0]\n",
    "      lst[1] = lst[1]+new_value[1]\n",
    "    lst[2]=0 if (lst[0] == 0) else lst[1]/lst[0]\n",
    "\n",
    "    return tuple(lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Socket Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname = \"nc\"\n",
    "port = 9999\n",
    "\n",
    "# create a DStream that will connect to hostname:port\n",
    "lines = ssc.socketTextStream(hostname, port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Transactions and Identify Debits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing transactions\n",
    "rawTxns = lines.map(lambda st: st.split(\",\")).map(lambda el: (el[0], el[1], float(el[2])))\n",
    "\n",
    "# filtering transactions for purchases only\n",
    "debitTxns = rawTxns.filter(lambda s: s[2] < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add the key to transactions to be able to compare with the amount average.\n",
    "\n",
    "In the real application it would be more natural to use the account id as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyedTxns = debitTxns.map(lambda s: (1, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Stream's State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amounts = debitTxns.map(lambda s: (1, (1, s[2], s[2])))\n",
    "meanAmount = amounts.updateStateByKey(updateFunc)\n",
    "\n",
    "meanAmount.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Transactions and State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedTxns = keyedTxns.join(meanAmount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Suspicious Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_factor = 1.33\n",
    "\n",
    "suspiciousTxns = joinedTxns.map(lambda v: v[1]).filter(lambda t: t[0][2] < t[1][2]*fraud_factor).map(lambda t: t[0])\n",
    "\n",
    "suspiciousTxns.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()             # starting the computation\n",
    "ssc.awaitTermination()  # waiting for the computation to terminate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate transaction source\n",
    "\n",
    "We will be receiving transaction data on port 9999.  There's a convenient utility in most Unix-like systems called `nc` (\"netcat\") that will take data from `stdin` and pipe it to a designated socket.\n",
    "\n",
    "> Note:  Windows systems should have a similar command called `ncat`.\n",
    "\n",
    "For our application, this will be `localhost` on port `9999`, of course.\n",
    "\n",
    "Open a new terminal and, if you're on Linux, issue the command\n",
    "\n",
    "``` \n",
    "nc localhost 9999\n",
    "```\n",
    "\n",
    "If you're on Mac, issue the command\n",
    "\n",
    "``` \n",
    "nc -lk 9999\n",
    "```\n",
    "\n",
    "Check your documentation if you're on Windows (I'll avoid the obligatory snide \"Windoze\" or other remark here).\n",
    "\n",
    "If there's an error, diagnose & correct it.  Otherwise, you should see the cursor on the next line, waiting for input on `stdin`.  Leave that process be for the moment; we're going to return to our program now and come back to it when we're ready to stream data.\n",
    "\n",
    "Copy and paste a large amount of transaction data from tx.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you saw how Spark Streaming's support for maintaining the stream state can be used to process continuously streamed data and handle it not only with ease, but also with nearly the same API and programming concepts as batch data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Streaming\").getOrCreate()\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# the duration is in seconds\n",
    "intervalDuration = 2\n",
    "\n",
    "ssc = StreamingContext(spark.sparkContext, intervalDuration)\n",
    "\n",
    "# checkpoint for backups\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "def updateFunc(new_values, last_state):\n",
    "\n",
    "    # handling the state that hasn't been created yet\n",
    "    state = (0, 0, 0) if (last_state is None)  else last_state\n",
    "    lst = list(state)\n",
    "    for new_value in new_values:\n",
    "      lst[0] = lst[0]+new_value[0]\n",
    "      lst[1] = lst[1]+new_value[1]\n",
    "    lst[2]=0 if (lst[0] == 0) else lst[1]/lst[0]\n",
    "\n",
    "    return tuple(lst)\n",
    "\n",
    "hostname = \"nc\"\n",
    "port = 9999\n",
    "\n",
    "# create a DStream that will connect to hostname:port\n",
    "lines = ssc.socketTextStream(hostname, port)\n",
    "\n",
    "# parsing transactions\n",
    "rawTxns = lines.map(lambda st: st.split(\",\")).map(lambda el: (el[0], el[1], float(el[2])))\n",
    "\n",
    "# filtering transactions for purchases only\n",
    "debitTxns = rawTxns.filter(lambda s: s[2] < 0)\n",
    "\n",
    "# we need to add the key to transactions to be able to compare with the amount mean\n",
    "# in the real application it would be more natural to use the account id as the key\n",
    "keyedTxns = debitTxns.map(lambda s: (1, s))\n",
    "\n",
    "# getting transaction amounts and updating the state holding amount mean\n",
    "amounts = debitTxns.map(lambda s: (1, (1, s[2], s[2])))\n",
    "meanAmount = amounts.updateStateByKey(updateFunc)\n",
    "meanAmount.pprint()\n",
    "\n",
    "# joining two streams with the purchase transactions and the mean\n",
    "joinedTxns = keyedTxns.join(meanAmount)\n",
    "\n",
    "fraud_factor = 1.33\n",
    "\n",
    "# getting suspicious purchases\n",
    "suspiciousTxns = joinedTxns.map(lambda v: v[1]).filter(lambda t: t[0][2] < t[1][2]*fraud_factor).map(lambda t: t[0])\n",
    "\n",
    "suspiciousTxns.pprint()\n",
    "\n",
    "ssc.start()             # starting the computation\n",
    "ssc.awaitTermination()  # waiting for the computation to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
