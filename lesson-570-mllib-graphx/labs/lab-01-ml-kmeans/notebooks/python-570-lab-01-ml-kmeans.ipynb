{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib\n",
    "\n",
    "In this lab, you will take your first steps using Spark's machine learning library, MLlib, specifically, the high-level ML pipeline API, to perform a k-means cluster analysis of transaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "\n",
    "1. Use the ML pipeline library to produce a k-means-based model.\n",
    "2. Use the generated model to predict the cluster to which given test transactions belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Overview\n",
    "\n",
    "We're going to add a tool to our fraud prevention tool belt and see if we can identify high-value transactions based on a person's transaction history.  In particular, we're going to identify 4 clusters of transactions:  low debits, high debits, low credits and high credits.  To be suspicious, any transaction amount must be either a high debit or a high credit.\n",
    "\n",
    "In order to keep it simple, we're only going to use a single feature of each transaction:  the amount.  In a real world system, there could be a few to a few hundred different features (merchant id, transaction description, transaction time of day, even dollar amounts, country of transaction origin, etc), which could be used to characterize suspicious activity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning process\n",
    "\n",
    "The machine learning process typically has the following phases:\n",
    "\n",
    "* **Featurization**:  identifying & quantifying the features that will be studied\n",
    "* **Training**:  a diverse enough data set is used to train a model to be used for future predictions\n",
    "* **Testing**:  a set of known data is presented to the trained model to see if it predicts well enough\n",
    "* **Production**:  the trained & tested model is used on real incoming data to make predictions\n",
    "\n",
    "Keep this process in mind as we begin our trip through machine learning land!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required packages\n",
    "\n",
    "First things first. Let's get some imports we'll need out of the way. Execute the following commands in your cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from math import pow\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "\n",
    "After reading data into an RDD, the first step in Machine Learning is \"featurizing\" the data.\n",
    "\n",
    "> Note:  In almost all machine learning environments, any data that will be used, regardless of its type, must be \"featurized\", which means to convert it into numeric values, one per \"feature\".  This gives rise to multidimensional spaces having anywhere from one to millions of dimensions.\n",
    "\n",
    "> Note:  Depending on the language used for Spark APIs features could be represnted by Vector (Scala, Java) or array (Python).\n",
    "\n",
    "In our case we want to cluster transaction amounts so the features will be represented by an array of a single element only.\n",
    "\n",
    "Let's get started with obtaining SparkSession and reading transactions in. Then transform transaction amounts into features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Clustering\").getOrCreate()\n",
    "\n",
    "file = \"/home/jovyan/Resources/tx.csv\"\n",
    "text = spark.sparkContext.textFile(file)\n",
    "\n",
    "txns = text.map(lambda st: st.split(\",\")).map(lambda el: (el[0], el[1], float(el[2])))\n",
    "\n",
    "amnts = txns.map(lambda v: array([v[2]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the data\n",
    "\n",
    "Now that our data is \"featurized\", we can train it.  It's as simple as this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCount = 10\n",
    "iterationCount = 20\n",
    "\n",
    "model = KMeans.train(amnts, clusterCount, iterationCount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `clusterCount` is the number of clusters we want our set to be clustered and `iterationCount` is the maximum number of iterations we want the algorithm to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "At this point, according to the canonical machine learning process, we'd use some carefully crafted test data to confirm that our model was adequately trained.  In lieu of that, we're just going to have a look at what the model came up with, which, in this case, is a collection of cluster centers based on our one-dimensional training data.  Each center is a point that represents the middle of a cluster that the model fit.\n",
    "\n",
    "Execute the following code in your cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster centers\")\n",
    "\n",
    "for c, value in enumerate(model.centers):\n",
    "    print(c, value[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're pulling the cluster centers adding their ids. They will be useful later on to find out what cluster a given value belongs to.\n",
    "\n",
    "The output should look something like the following:\n",
    "\n",
    "````\n",
    "Cluster centers\n",
    "0 -5.50525267994\n",
    "1 1440.24\n",
    "2 -819.5775\n",
    "3 825.062\n",
    "4 259.11030303\n",
    "5 -192.826097561\n",
    "6 2209.03\n",
    "7 -36.3006958763\n",
    "8 -362.944444444\n",
    "9 -89.8272093023\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first value in the tuple is the index of the cluster center in the `clusterCenters`, which also is the cluster id.  The second value is the value of center.  This means that we have a high debit of around \\$275 with cluster id `3`, a low debit of around $22 with id `0`, a low credit of around \\$510 with id `1`, and a high credit of around \\$1697 with id `2`, which, given the 1200+ transactions, makes a reasonable amount of sense.\n",
    "\n",
    "Here are the raw transaction amounts with cluster centers overlaid (blue diamonds are raw transaction amounts, red squares are cluster centers):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cluster Centers with Raw Transaction Data](../Resources/img/centersx.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another visualization taking into account the cluster ids.  Raw transaction amounts (again, blue diamonds) are along the x-axis at y = 0, and cluster centers (red squares) are offset along the y-axis at each cluster center's id value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cluster Centers and Cluster IDs with Raw Transaction Data](../Resources/img/centersy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visualization not only lets you know the cluster center values, but also their ids for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model\n",
    "\n",
    "Now that our testing proves our model reasonably well, let's throw some data at the model to see what it says about it.  Execute the following code, which creates some extremely varied transaction amounts from -\\$5,000 to \\$5,000, then presents them to the model, showing their resultant predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the amounts for clustering: +- 5*10^i, i=0..3\n",
    "vals = spark.sparkContext.parallelize(range(4)).map(lambda i: pow(10, i)).flatMap(lambda x: array([-5*x, 5*x])).map(lambda v: array([v]))\n",
    "\n",
    "# predict the clusters for the amounts\n",
    "clsd = vals.map(lambda s: (s, model.predict(s)))\n",
    "\n",
    "print(\"Predictions\")\n",
    "for value, c in clsd.collect():\n",
    "    print(value[0], c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your results should look like the following:\n",
    "\n",
    "```\n",
    "Predictions\n",
    "-5.0 0\n",
    "5.0 0\n",
    "-50.0 7\n",
    "50.0 0\n",
    "-500.0 8\n",
    "500.0 4\n",
    "-5000.0 2\n",
    "5000.0 6\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that a debit & a debit of \\$5 & \\$50 all belong to cluster id `0`, which we called \"low debit\".  Interesting.  Maybe we should rename \"low debit\" to be \"normal transaction\", since they're not all debits.  Thus is the way of data analysis.  Next, our model says that a debit of \\$500 belongs to cluster id `3`, which was \"high debit\".  A HA!  Fraud alert!  Our model, rightfully so, suggests that we should investigate this transaction.  Same goes for the \\$5,000 debit, also as it rightfully should!  Our model also classified the \\$5,000 credit as \"high credit\" (in cluster id `2`), which should also be investigated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note:  Printing transactions to the screen is not a real world way of notifying a company of fraudulent transactions.  However, since the model returns an `RDD`, we can do whatever we want with it, including sending it to another system, writing it to a file that some other fraud investigation process may be monitoring, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you saw how you can leverage Spark's sophisticated machine learning processing. While we used k-means for this example, there are many other machine learning algorithms included in Spark MLlib out of the box.  All of the sudden, becoming a bonafide data scientists seems within reach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "#### FInd the \"optimal\" number of clusters for transaction amounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from math import pow\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Clustering\").getOrCreate()\n",
    "\n",
    "\n",
    "file = \"/home/jovyan/Resources/tx.csv\"\n",
    "text = spark.sparkContext.textFile(file)\n",
    "\n",
    "txns = text.map(lambda st: st.split(\",\")).map(lambda el: (el[0], el[1], float(el[2])))\n",
    "\n",
    "# get the amounts for clustering training\n",
    "amnts = txns.map(lambda v: array([v[2]]))\n",
    "\n",
    "clusterCount = 10\n",
    "iterationCount = 20\n",
    "model = KMeans.train(amnts, clusterCount, iterationCount)\n",
    "\n",
    "print(\"Cluster centers\")\n",
    "for c, value in enumerate(model.centers):\n",
    "    print(c, value[0])\n",
    "\n",
    "# generate the amounts for clustering: +- 5*10^i, i=0..3\n",
    "vals = spark.sparkContext.parallelize(range(4)).map(lambda i: pow(10, i)).flatMap(lambda x: array([-5*x, 5*x])).map(lambda v: array([v]))\n",
    "\n",
    "# predict the clusters for the amounts\n",
    "clsd = vals.map(lambda s: (s, model.predict(s)))\n",
    "\n",
    "print(\"Predictions\")\n",
    "for value, c in clsd.collect():\n",
    "    print(value[0], c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the solution above the number of desired clusters (`k`) is passed to the algorithm.\n",
    "\n",
    "How do we know that this number is optimal? Well, we don't but what we can do is assess how good it is by calculating the total distance of all clustered points from their cluster centers and compare it between different `k`s. This total distance is called `Within Set Sum of Squared Error (WSSSE)` and it's used by the algorithm to evaluate the quality of the clustering for a given `k`. You can easily compute `WSSSE` yourself or you can obtain it by calling `computeCost` function on the model. \n",
    "\n",
    "So, the lower `WSSSE` the better, but not quite as you can easily see that `WSSSE` is 0 when the number of desired clusters is the same as the number of clustered points. You cannot beat that!\n",
    "\n",
    "What we're looking for, instead, is the point where `WSSSE` starts decreasing slowly with increased number of `k`.\n",
    "\n",
    "When you plot `WSSSE` the optimal `k` is where there is an \"elbow\". In our case 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from math import pow\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Clustering\").getOrCreate()\n",
    "\n",
    "\n",
    "file = \"/home/jovyan/Resources/tx.csv\"\n",
    "text = spark.sparkContext.textFile(file)\n",
    "\n",
    "txns = text.map(lambda st: st.split(\",\")).map(lambda el: (el[0], el[1], float(el[2])))\n",
    "\n",
    "# get the amounts for clustering training\n",
    "amnts = txns.map(lambda v: array([v[2]]))\n",
    "\n",
    "iterationCount = 20\n",
    "\n",
    "print(\"Within Set Sum of Squared Errors\")\n",
    "\n",
    "for clusterCount in range(1, 21):\n",
    "\n",
    "  model = KMeans.train(amnts, clusterCount, iterationCount)\n",
    "\n",
    "  wssse = model.computeCost(amnts)\n",
    "  print(str(clusterCount) + \"\\t\" + str(round(wssse)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Optimal Cluster Count](../Resources/img/optimal_cluster_count.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
