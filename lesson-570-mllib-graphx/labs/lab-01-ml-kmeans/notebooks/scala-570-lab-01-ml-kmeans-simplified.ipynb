{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centers\n",
      "(0,137.71837801546664)\n",
      "(1,-15.051251378631923)\n",
      "(2,2209.030029296875)\n",
      "(3,825.0619995117188)\n",
      "(4,-362.9444410536024)\n",
      "(5,1440.239990234375)\n",
      "(6,-819.5774993896484)\n",
      "(7,-191.42642865862163)\n",
      "(8,389.8246154785156)\n",
      "(9,-72.59154530144556)\n",
      "Predictions\n",
      "(-5.0,1)\n",
      "(5.0,1)\n",
      "(-50.0,9)\n",
      "(50.0,1)\n",
      "(-500.0,4)\n",
      "(500.0,8)\n",
      "(-5000.0,6)\n",
      "(5000.0,2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@15bbf8d6\n",
       "file = /home/jovyan/Resources/tx.csv\n",
       "text = /home/jovyan/Resources/tx.csv MapPartitionsRDD[1] at textFile at <console>:37\n",
       "txns = MapPartitionsRDD[3] at map at <console>:39\n",
       "amnts = MapPartitionsRDD[4] at map at <console>:42\n",
       "clusterCount = 10\n",
       "iterationCount = 20\n",
       "model = org.apache.spark.mllib.clustering.KMeansModel@2bb83aaf\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vals: org.apache.spark.rdd.RDD[org.apache....\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.clustering.KMeansModel@2bb83aaf"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"Scala Machine Learning Clustering\").getOrCreate()\n",
    "\n",
    "val file = \"/home/jovyan/Resources/tx.csv\"\n",
    "val text = spark.sparkContext.textFile(file)\n",
    "\n",
    "val txns = text.map(s => s.split(\",\")).map(a => a(2).toFloat)\n",
    "\n",
    "// get the amounts for clustering training, the amounts need to be vectorized\n",
    "val amnts = txns.map(v => Vectors.dense(v)).cache\n",
    "\n",
    "val clusterCount = 10\n",
    "val iterationCount = 20\n",
    "\n",
    "// training the model\n",
    "val model = KMeans.train(amnts, clusterCount, iterationCount)\n",
    "\n",
    "println(\"Cluster centers\")\n",
    "\n",
    "// zipping with index to get the cluster label\n",
    "model.clusterCenters.zipWithIndex.map(c => (c._2, c._1(0))).foreach(println(_))\n",
    "\n",
    "// generate the amounts for clustering: +- 5*10^i, i=0..3\n",
    "val vals = sc.parallelize(0 to 3).map(i => Math.pow(10, i)).flatMap(x => Array(-5*x, 5*x)).map(v => Vectors.dense(v))\n",
    "\n",
    "// predict the clusters for the amounts\n",
    "val clsd = vals.map(s => (s(0), model.predict(s)))\n",
    "\n",
    "println(\"Predictions\")\n",
    "\n",
    "clsd.collect.foreach(println(_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "#### FInd the \"optimal\" number of clusters for transaction amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors\n",
      "1\t22058151\n",
      "2\t10992591\n",
      "3\t7954986\n",
      "4\t6712829\n",
      "5\t3361231\n",
      "6\t2472643\n",
      "7\t1898504\n",
      "8\t1416450\n",
      "9\t1150353\n",
      "10\t766466\n",
      "11\t834030\n",
      "12\t509116\n",
      "13\t460154\n",
      "14\t353169\n",
      "15\t331615\n",
      "16\t285519\n",
      "17\t236213\n",
      "18\t194126\n",
      "19\t195423\n",
      "20\t161736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@15bbf8d6\n",
       "file = /home/jovyan/Resources/tx.csv\n",
       "text = /home/jovyan/Resources/tx.csv MapPartitionsRDD[64] at textFile at <console>:45\n",
       "txns = MapPartitionsRDD[66] at map at <console>:47\n",
       "amnts = MapPartitionsRDD[67] at map at <console>:50\n",
       "iterationCount = 20\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"Scala Machine Learning Clustering\").getOrCreate()\n",
    "\n",
    "val file = \"/home/jovyan/Resources/tx.csv\"\n",
    "val text = spark.sparkContext.textFile(file)\n",
    "\n",
    "val txns = text.map(s => s.split(\",\")).map(a => a(2).toFloat)\n",
    "\n",
    "// get the amounts for clustering training, the amounts need to be vectorized\n",
    "val amnts = txns.map(v => Vectors.dense(v)).cache\n",
    "\n",
    "val iterationCount = 20\n",
    "\n",
    "println(\"Within Set Sum of Squared Errors\")\n",
    "for (clusterCount <- 1 to 20) {\n",
    "  val model = KMeans.train(amnts, clusterCount, iterationCount)\n",
    "\n",
    "  val wssse = model.computeCost(amnts)\n",
    "  println(clusterCount + \"\\t\" + Math.round(wssse))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for an \"elbow\" in WSSSE graph. We think the optimal number of clusters is 9."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
