{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib\n",
    "\n",
    "In this lab, you will take your first steps using Spark's machine learning library, MLlib, specifically, the high-level ML pipeline API, to perform a k-means cluster analysis of transaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "\n",
    "1. Use the ML pipeline library to produce a k-means-based model.\n",
    "2. Use the generated model to predict the cluster to which given test transactions belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "We're going to add a tool to our fraud prevention tool belt and see if we can identify high-value transactions based on a person's transaction history.  In particular, we're going to identify 4 clusters of transactions:  low debits, high debits, low credits and high credits.  To be suspicious, any transaction amount must be either a high debit or a high credit.\n",
    "\n",
    "In order to keep it simple, we're only going to use a single feature of each transaction:  the amount.  In a real world system, there could be a few to a few hundred different features (merchant id, transaction description, transaction time of day, even dollar amounts, country of transaction origin, etc), which could be used to characterize suspicious activity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning process\n",
    "\n",
    "The machine learning process typically has the following phases:\n",
    "\n",
    "* **Featurization**:  identifying & quantifying the features that will be studied\n",
    "* **Training**:  a diverse enough data set is used to train a model to be used for future predictions\n",
    "* **Testing**:  a set of known data is presented to the trained model to see if it predicts well enough\n",
    "* **Production**:  the trained & tested model is used on real incoming data to make predictions\n",
    "\n",
    "Keep this process in mind as we begin our trip through machine learning land!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required packages\n",
    "\n",
    "First things first.  Let's get some `import`s we'll need out of the way.  Execute the following commands in your shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml._\n",
    "import org.apache.spark.ml.clustering._\n",
    "import org.apache.spark.ml.feature._\n",
    "import scala.math._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now on to the good stuff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ML pipeline\n",
    "\n",
    "The high-level ML API consists of define `Pipeline`s, which are comprised of an ordered collection of `PipelineStage`s.  There are two types of `PipelineStage`s:  `Transformer`s & `Estimator`s.  A `Transformer` is, not surprisingly, used to transform data as it flows through the `Pipeline`; its primary method is `transform(data: DataFrame): DataFrame`.  An `Estimator[M]` is used to fit models to data; its primary method is `fit(data: DataFrame): M`, where `M` is the parameterized type of the particular  `Estimator`'s `Model`.  For this lab, our `Estimator` will be `KMeans`, which `extends Estimator[KMeansModel]`.\n",
    "\n",
    "> Note:  The pipeline API is built to use `DataFrame`s, which is Spark's SQL API.  That's why you see these methods defined in terms of that instead of `RDD`s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation\n",
    "\n",
    "The first stage of our pipeline is a convenient `Transformer` that converts the incoming `DataFrame` into a feature vector; it's called a `VectorAssembler`.\n",
    "\n",
    "> Note:  In almost all machine learning environments, any data that will be used, regardless of its type, must be \"featurized\", which means to convert it into numeric values, one per \"feature\".  This gives rise to multidimensional spaces having anywhere from one to millions of dimensions.  A `Vector` is simply a type that includes an index of type `Int` and values of type `Double`, one per feature.\n",
    "\n",
    "Our ML pipeline, then, will take an input of a `DataFrame` of transactions, then, in the first stage, transform it into a `Vector` containing a single feature.\n",
    "\n",
    "Our incoming transaction data will be our test data file, `lesson-5xx-resources/tx.csv`.  \n",
    "Execute the following lines in your shell, which will read the transaction data into a `DataFrame`.\n",
    "First let's define a case class to hold the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Tx(date: String, desc: String, amount: Double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's obtain the data as a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val training = sc.textFile(\"/home/jovyan/Resources/tx.csv\")\n",
    "    .map(_.split(\",\"))\n",
    "    .map(x => Tx(x(0), x(1), x(2).toDouble))\n",
    "    .toDF\n",
    "    .cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're doing here is pretty simple:  reading the CSV file, splitting on commas, converting each line into a `Tx` object, converting the resulting `RDD` into the required `DataFrame`, then caching it in the cluster (in case we need to read it again).\n",
    "\n",
    "Now that we have our source data, let's return to our `VectorAssembler`, where we'll be featurizing our data.  All we have to do is to tell the `VectorAssembler` which columns from our `DataFrame` represent our features, and what the output feature column name will be.  In our case it's just the single column `amount`, and we'll use the output column name `features` (even though that's the default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val assembler = new VectorAssembler()\n",
    "    .setInputCols(Array(\"amount\"))\n",
    "    .setOutputCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note:  the output column, while a single column, will always contain multivalued contents.  In this case, it will be a list of size one for the one column `amount`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation (fitting the model)\n",
    "\n",
    "The next stage of our ML pipeline will be to fit a model based on our data.  As mentioned before, we're going to use k-means to identify the 4 clusters of transactions (high debit, low debit, low credit, high credit).  Execute the following lines in your shell to define your `KMeans` estimator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val k = 4\n",
    "val maxIterations = 10000\n",
    "val km = new KMeans().setMaxIter(maxIterations).setK(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all `Transformer`s & `Estimator`s take parameters to tune their results and/or performance.  The `KMeans` parameters should look familiar to those who've used it before.  `setK` sets the `k` value, which is the number of clusters the data should be divided into.  `setMaxIter` sets the maximum number of iterations that the model will use while fitting the data.  There is another parameter set via `setTol` (unused in this lab), that is used to set the tolerance or \"epsilon\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare the ML pipeline\n",
    "\n",
    "Now that our two stages are defined, let's define our `Pipeline`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pl = new Pipeline().setStages(Array(assembler, km))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it!  We're simply instantiating a new, empty `Pipeline`, then setting its ordered stages.  Now, we're ready to roll.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the data\n",
    "\n",
    "Now that our `Pipeline` is defined, we can start using our training data.  It's as simple as this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val model = pl.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply ask our `Pipeline` to `fit` the training data and return to us a `Model`!  Easy-peasy, thanks to Spark!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "At this point, according to the canonical machine learning process, we'd use some carefully crafted test data to confirm that our model was adequately trained.  In lieu of that, we're just going to have a look at what the model came up with, which, in this case, is a collection of cluster centers based on our one-dimensional training data.  Each center is a point that represents the middle of a cluster that the model fit.\n",
    "\n",
    "Execute the following code in your shell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val centers = model.stages(1).asInstanceOf[KMeansModel].clusterCenters\n",
    "println(\"Cluster Centers:\")\n",
    "centers.zipWithIndex.map(c => (c._2,c._1(0))).foreach(println(_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're pulling out the actual `KMeansModel` inside the `Pipeline` at the second stage (at index `1`, since stage indexes are zero-based).  Then, we're using a tiny bit of Scala magic to see what the cluster ids and values are.  The output should look something like the following:\n",
    "\n",
    "``` \n",
    "centers: Array[org.apache.spark.mllib.linalg.Vector] = Array([-21.980326906957238], [510.7238888888888], [1696.5033333333333], [-274.7556862745098])\n",
    "Cluster Centers:\n",
    "(0,-21.980326906957238)\n",
    "(1,510.7238888888888)\n",
    "(2,1696.5033333333333)\n",
    "(3,-274.7556862745098)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first value in the tuple is the index of the cluster center in the `clusterCenters`, which also is the cluster id.  The second value is the value of center.  This means that we have a high debit of around \\$275 with cluster id `3`, a low debit of around $22 with id `0`, a low credit of around \\$510 with id `1`, and a high credit of around \\$1697 with id `2`, which, given the 1200+ transactions, makes a reasonable amount of sense.\n",
    "\n",
    "Here are the raw transaction amounts with cluster centers overlaid (blue diamonds are raw transaction amounts, red squares are cluster centers):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cluster Centers with Raw Transaction Data](../Resources/img/centersx.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another visualization taking into account the cluster ids.  Raw transaction amounts (again, blue diamonds) are along the x-axis at y = 0, and cluster centers (red squares) are offset along the y-axis at each cluster center's id value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cluster Centers and Cluster IDs with Raw Transaction Data](../Resources/img/centersy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visualization not only lets you know the cluster center values, but also their ids for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model\n",
    "\n",
    "Now that our testing proves our model reasonably well, let's throw some data at the model to see what it says about it.  Execute the following code, which creates some extremely varied transaction amounts from -\\$5,000 to \\$5,000, then presents them to the model, showing their resultant predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Val(amount:Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val seq = List.range(0,4).map(pow(10, _)).flatMap(x => Array(Val(-5*x),Val(5*x)))\n",
    "val test = sqlContext.createDataFrame(seq)\n",
    "\n",
    "val result = model.transform(test)\n",
    "println(\"Predictions:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your results should look like the following:\n",
    "\n",
    "``` \n",
    "defined class Val\n",
    "seq: List[Val] = List(Val(-5.0), Val(5.0), Val(-50.0), Val(50.0), Val(-500.0), Val(500.0), Val(-5000.0), Val(5000.0))\n",
    "test: org.apache.spark.sql.DataFrame = [amount: double]\n",
    "result: org.apache.spark.sql.DataFrame = [amount: double, features: vector, prediction: int]\n",
    "Predictions:\n",
    "+-------+---------+----------+\n",
    "| amount| features|prediction|\n",
    "+-------+---------+----------+\n",
    "|   -5.0|   [-5.0]|         0|\n",
    "|    5.0|    [5.0]|         0|\n",
    "|  -50.0|  [-50.0]|         0|\n",
    "|   50.0|   [50.0]|         0|\n",
    "| -500.0| [-500.0]|         3|\n",
    "|  500.0|  [500.0]|         1|\n",
    "|-5000.0|[-5000.0]|         3|\n",
    "| 5000.0| [5000.0]|         2|\n",
    "+-------+---------+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that a debit & a debit of \\$5 & \\$50 all belong to cluster id `0`, which we called \"low debit\".  Interesting.  Maybe we should rename \"low debit\" to be \"normal transaction\", since they're not all debits.  Thus is the way of data analysis.  Next, our model says that a debit of \\$500 belongs to cluster id `3`, which was \"high debit\".  A HA!  Fraud alert!  Our model, rightfully so, suggests that we should investigate this transaction.  Same goes for the \\$5,000 debit, also as it rightfully should!  Our model also classified the \\$5,000 credit as \"high credit\" (in cluster id `2`), which should also be investigated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note:  Printing transactions to the screen is not a real world way of notifying a company of fraudulent transactions.  However, since the model returns a `DataFrame`, we can do whatever we want with it, including sending it to another system, writing it to a file that some other fraud investigation process may be monitoring, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you saw how you can leverage Spark's high-level ML pipeline API to do arbitrarily sophisticated machine learning processing.  While we used k-means for this example, there are many other machine learning algorithms included in Spark MLlib out of the box.  All of the sudden, becoming a bonafide data scientists seems within reach!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
