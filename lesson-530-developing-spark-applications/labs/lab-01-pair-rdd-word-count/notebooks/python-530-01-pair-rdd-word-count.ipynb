{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Spark Applications: PairRDDFunctions\n",
    "\n",
    "In this lab, you will take your first steps using PairRDDFunctions using Jupyter Notebook.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "Use the SparkContext to bootstrap RDDs of Tuples in order to use PairRDDFunctions. Use the PairRDDFunctions to more easily calculate values in the RDD.\n",
    "\n",
    "### Consider Tuples\n",
    "\n",
    "Scala's support for arbitrary pairs (and triplets, quadruplets, and so on) of data is reflected in Spark. If an RDD's element is of a Tuple type, then, through Spark implicits, that RDD gets enriched with PairRDDFunctions, which provide convenient means of performing calculations on the Tuples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In order to work with data using Spark, we need to get our hands on a dataset.  Spark calls such a dataset a \"resilient distributed dataset\", or `RDD`.  There are several ways to get your hands on an `RDD`.  In most cases, however, you're going to use a `SparkContext` to do that.\n",
    "\n",
    "Let's start with something simple:  a plain, ol' Scala collection of the integers from 1 to 10,000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a SparkSession\n",
    "\n",
    "In order to work with Spark's SQL support, we need to first get our hands on a special context called `SparkSession`.  \n",
    "The SparkSession class is the entry point into all functionality in Spark. \n",
    "\n",
    "> Note: as of Spark 2.0, SparkSession replaced SqlContext. However, we could still use SqlContext as it's being kept for backward compatibility.\n",
    "\n",
    "We'll use SparkSession.builder to create a SparkSession. SparkSession.builder lets you define you application name and it also lets you set various parameters in the Spark config, although there is no need to do so for our simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Pair RDD Functions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if Spark Context works\n",
    "\n",
    "Create an RDD consisting of numbers from 0 to 999. Take a random sample of 5 numbers (without replacement - first parameter to takeSample function).\n",
    "\n",
    "You should see the output consisting of 5 random numbers from the initial range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(range(1000))\n",
    "\n",
    "rdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read external file into RDD\n",
    "\n",
    "You'll be using a file sherlock-holmes.txt. You need to retrieve it from the container folder (/home/jovyan/Resources) mounted to host machine directory - see instructions about setting up the Docker container to run Jupyter Notebook.\n",
    "\n",
    "Read text into RDD. \n",
    "\n",
    "Make sure that the file was read correctly by printing the RDD element count (here the number of lines in the file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/jovyan/Resources/sherlock-holmes.txt\"\n",
    "\n",
    "lines = spark.sparkContext.textFile(file)\n",
    "\n",
    "print(lines.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Count the words\n",
    "\n",
    "Before we get to controlling the degree of parallelism let's make sure we're able to count the words and find the one with the maximum count.\n",
    "\n",
    "Split the lines into individual words, for each word make a tuple, where the first element is the word itself and the second element is the initial count 1. Finally reduce by key obtaining the RDD with unique words and the total counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "words = lines.flatMap(lambda w : re.split('[ \\],.:;?!\\-@#\\(\\)\\\\\\*\\\"\\/]*', w)).map(lambda w : w.lower())\n",
    "\n",
    "words.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll consider two equivalent ways to accomplish our goal\n",
    "They will be using PairRDDFunctions, which operate on data organized as key-value pairs. The functions typically agregate values for the keys.\n",
    "\n",
    "### <font color=blue>Use reduceByKey</font>\n",
    "\n",
    "One of the most useful PairRDDFunctions is reduceByKey, which transforms a pair RDD anto another pair RDD, where the values for each key are aggregated using the supplied reduce function.\n",
    "\n",
    "In our case, the pair's key is the word, and the value is the count. The initial value for each word is 1.\n",
    "\n",
    "The reduce function is a simple addition as we'll be adding the counts of the same words. The values will be aggregated, finally resulting in the total count for each distinct word.\n",
    "\n",
    "Let's transform our words RDD into initial pairs RDD and while we're at it convert words into lower case for better accuracy of the counts.\n",
    "\n",
    "Next we'll be applying reduceByKey function yielding the total word counts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "pairs = words.map(lambda w : (w, 1))\n",
    "\n",
    "counts = pairs.reduceByKey(add)\n",
    "\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert RDD into Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = counts.collectAsMap()\n",
    "\n",
    "print(\"if  count \" + str(map[\"if\"]))\n",
    "print(\"and count \" + str(map[\"and\"]))\n",
    "print(\"but count \" + str(map[\"but\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Use countByKey</font>\n",
    "\n",
    "It just so happens that PairRDDFunctions has a method called `countByKey`, which returns a Map of counts of the occurrence of each key among the pairs. \n",
    "\n",
    "This function also requires a pair RDD. However, since it counts the occurrences of the keys it doesn't really matter what the values are (even the value type is irrelevant). Therefore we'll be using None as opposed to 1 as the initial value of the pairs.\n",
    "\n",
    "Note that unlike `reduceByKey` there is no need to collect RDD values into a Map as `countByKey` does it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = words.map(lambda w : (w, None))\n",
    "\n",
    "counts = pairs.countByKey()\n",
    "\n",
    "print(\"if  count \" + str(counts[\"if\"]))\n",
    "print(\"and count \" + str(counts[\"and\"]))\n",
    "print(\"but count \" + str(counts[\"but\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other interesting things about Sherlock Holmes\n",
    "\n",
    "#### Find the word with the highest count\n",
    "\n",
    "We'll use `reduce` function, which out of two pair elements returns the one with the higher count (note the way we access the individual elements of pairs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxCount(r, c):\n",
    "  if (r[1] > c[1]):\n",
    "    return r\n",
    "  else:\n",
    "    return c\n",
    "\n",
    "pairs = words.map(lambda w : (w, 1))\n",
    "counts = pairs.reduceByKey(add)\n",
    "\n",
    "max = counts.reduce(lambda r, c: getMaxCount(r, c))\n",
    "\n",
    "print(\"word with max count \" + str(max))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the longest word\n",
    "\n",
    "We'll use `reduce` function, which out of two pair elements returns the one with the longer key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLength(r, c):\n",
    "  if (len(r[0]) > len(c[0])):\n",
    "    return r\n",
    "  else:\n",
    "    return c\n",
    "\n",
    "pairs = words.map(lambda w : (w, 1))\n",
    "counts = pairs.reduceByKey(add)\n",
    "\n",
    "max = counts.reduce(lambda r, c: getMaxLength(r, c))\n",
    "\n",
    "print(\"word with max count \" + str(max))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Assignment\n",
    "\n",
    "#### Find all anagrams among words\n",
    "\n",
    "Can you write an algorithm that finds all the anograms (a word, phrase, or name formed by rearranging the letters of another, such as `cinema`, formed from `iceman`) in the text?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this lab, we saw how an RDD of Tuple types receives via an implicit additional methods from PairRDDFunctions to make calculating certain values easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code as one continuous segment\n",
    "\n",
    "Here is a possible solution to the discussion above written as one code segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Pair RDD Functions\").getOrCreate()\n",
    "\n",
    "file = \"/home/jovyan/Resources/sherlock-holmes.txt\"\n",
    "text = spark.sparkContext.textFile(file)\n",
    "\n",
    "import re\n",
    "\n",
    "# use regular expression to split lines into words and translate them into lower case\n",
    "words = text.flatMap(lambda w : re.split('[ \\],.:;?!\\-@#\\(\\)\\\\\\*\\\"\\/]*', w)).map(lambda w : w.lower())\n",
    "\n",
    "# map each word to a pair with word as key and initial count 1\n",
    "# then reduce by key getting the total counts for each word\n",
    "pairs = words.map(lambda w : (w, 1)).reduceByKey(add)\n",
    "\n",
    "# collect pairs into dictionary for random access\n",
    "list = pairs.collectAsMap()\n",
    "\n",
    "# print the counts for given words\n",
    "print(\"if  count \" + str(list[\"if\"]))\n",
    "print(\"and count \" + str(list[\"and\"]))\n",
    "print(\"but count \" + str(list[\"but\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for Bonus Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Pair RDD Functions\").getOrCreate()\n",
    "\n",
    "file = \"/home/jovyan/Resources/sherlock-holmes.txt\"\n",
    "lines = spark.sparkContext.textFile(file)\n",
    "\n",
    "import re\n",
    "\n",
    "# use regular expression to split lines into words and translate them into lower case\n",
    "words = lines.flatMap(lambda w : re.split('[ \\],.:;?!\\-@#\\(\\)\\\\\\*\\\"\\/]*', w)).map(lambda w : w.lower())\n",
    "\n",
    "# select words longer than 6 letters and make them distinct\n",
    "dstnc = words.filter(lambda w : len(w) > 6).distinct()\n",
    "\n",
    "# we're creating a pair RDD where the key is the original word sorted by letters \n",
    "# and the initial value is a singleton list with the word as an element\n",
    "# any two words sharing the same sorted forms are anagrams\n",
    "\n",
    "pairs = dstnc.map(lambda w : (''.join(sorted(w)), [w]))\n",
    "\n",
    "# the reduce function is concatenating lists of words\n",
    "# map is dropping the keys as they're no loner needed\n",
    "# filter is returning only those lists, which are longer than 1 (true anagrams)\n",
    "\n",
    "angrs = pairs.reduceByKey(lambda w1, w2 : w1 + w2).map(lambda p : p[1]).filter(lambda w : len(w) > 1)\n",
    "\n",
    "for a in angrs.collect() :\n",
    "  print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
