{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing with RDDs:  Parallelism\n",
    "\n",
    "In this lab, you will take your first steps using RDDs in Spark using Jupyter Notebook.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Create the `SparkContext` to bootstrap `RDD`s.\n",
    "2. Use the `RDD`s to explore parallelism & its benefits when processing data.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This lab assumes that the student is familiar with the course environment, in particular, Jupyter Notebook.\n",
    "\n",
    "### Consider parallelism\n",
    "\n",
    "Whenever we process big data, we need to consider the degree to which we'll be able to use parallelism.  The general idea is that if we can break our job down into smaller and smaller chunks that can execute in parallel, we should perform better.  In this lab, we're going to test that theory by methodically increasing the degree of parallelism when performing a word count calculation to get the most frequently used word in some text file.\n",
    "\n",
    "### Create Spark Context\n",
    "\n",
    "Spark context establishes a connection to a Spark execution environment.\n",
    "For example, it's used to create RDDs by reading files or by parallelizing lists of objects.\n",
    "You need to create it first as follows (make sure pyspark library is imported).\n",
    "\n",
    "```\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "```\n",
    "For a given notebook it needs to be created only once.\n",
    "\n",
    "### Check if Spark Context works\n",
    "\n",
    "Create an RDD consisting of numbers from 0 to 999.\n",
    "Take a random sample of 5 numbers (without replacement - first parameter to takeSample function)\n",
    "\n",
    "```\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)\n",
    "```\n",
    "\n",
    "You should see the output consisting of 5 random numbers from the initial range.\n",
    "\n",
    "```\n",
    "[492, 777, 372, 844, 131]\n",
    "```\n",
    "\n",
    "### Read external file into RDD\n",
    "\n",
    "You'll be using a file sherlock-holmes.txt. You need to retrieve it from the container folder mounted to host machine directory (/data) - see instructions about setting up the Docker container to run Jupyter Notebook.\n",
    "\n",
    "```\n",
    "file = \"/data/sherlock-holmes.txt\"\n",
    "```\n",
    "\n",
    "Read text into RDD and cache it using persist. Caching greatly improves performance when RDDs are used more than once, which will be done in this excercise (if not cached the file would be read and parsed each time the RDD is used).\n",
    "\n",
    "Make sure that the file was read correctly by printing the RDD element count (here the number of lines in the file).\n",
    "\n",
    "```\n",
    "text = sc.textFile(file).persist()\n",
    "print(text.count())\n",
    "```\n",
    "\n",
    "### Count the words\n",
    "\n",
    "Before we get to controlling the degree of parallelism let's make sure we're able to count the words and find the one with the maximum count.\n",
    "\n",
    "Split the lines into individual words, for each word make a tuple, where the first element is the word itself and the second element is the initial count 1. Finally reduce by key obtaining the RDD with unique words and the total counts.\n",
    "\n",
    "```\n",
    "from operator import add\n",
    "\n",
    "words = text.flatMap(lambda w : w.split(' '))\n",
    "dict = words.map(lambda w : (w, 1))\n",
    "counts = dict.reduceByKey(add)\n",
    "\n",
    "counts.take(10)\n",
    "```\n",
    "\n",
    "### Fnd the word with the maximum count\n",
    "\n",
    "\n",
    "Let's define the function first which takes two tuples (word and count) as arguments and returns the one with the bigger count.\n",
    "\n",
    "```\n",
    "def getMax(r, c):\n",
    "  if (r[1] > c[1]):\n",
    "    return r\n",
    "  else:\n",
    "    return c\n",
    "```\n",
    "\n",
    "Use the function to reduce the RDD to obtain the word with the biggest count.\n",
    "\n",
    "```\n",
    "max = counts.reduce(lambda r, c: getMax(r, c))\n",
    "\n",
    "print(max)\n",
    "```\n",
    "\n",
    "### Control the degree of parallelism\n",
    "\n",
    "Until now we'have been doing a fairly straightforward word count, returning the most frequently used word. \n",
    "\n",
    "Now we want to execute the procedure in a loop, each time increasing the number of partitions participating in the execution thus increasing the level of parallelism.\n",
    "\n",
    "This will be accomplished by repartitioning the original RDD with cached lines of text through transforming it to another RDD with `partitionCount` partitions.\n",
    "\n",
    "For any RDD the number of partitions is given by `getNumPartitions`.\n",
    "\n",
    "```\n",
    "for partitionCount in range(2, 9, 2):\n",
    "\n",
    "    rept = text.repartition(partitionCount)\n",
    "    print(rept.getNumPartitions())\n",
    "```\n",
    "\n",
    "After repartitioning we follow with the word counting procedure using the initial RDD after transformation.\n",
    "\n",
    "We want to capture the execution time in millisecods after each iteration.\n",
    "\n",
    "```\n",
    "('the', 71744) partition count 2 time 460\n",
    "('the', 71744) partition count 4 time 156\n",
    "('the', 71744) partition count 6 time 296\n",
    "('the', 71744) partition count 8 time 263\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "What do you notice about our timings?  Do they confirm or refute our hypothesis that more parallism generally means better performance?  If so, great!  If not, what do you suppose are other factors that are influencing our outcome?  Discuss with your instructor and classmates!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[823, 365, 170, 496, 368]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128457\n"
     ]
    }
   ],
   "source": [
    "file = \"/data/sherlock-holmes.txt\"\n",
    "text = sc.textFile(file).persist()\n",
    "\n",
    "print(text.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 6139),\n",
       " ('Project', 205),\n",
       " ('EBook', 5),\n",
       " ('of', 39169),\n",
       " ('Sir', 30),\n",
       " ('Arthur', 18),\n",
       " ('Conan', 3),\n",
       " ('in', 19515),\n",
       " ('series', 88),\n",
       " ('', 69285)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "words = text.flatMap(lambda w : w.split(' '))\n",
    "dict = words.map(lambda w : (w, 1))\n",
    "counts = dict.reduceByKey(add)\n",
    "\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 71744)\n"
     ]
    }
   ],
   "source": [
    "def getMax(r, c):\n",
    "  if (r[1] > c[1]):\n",
    "    return r\n",
    "  else:\n",
    "    return c\n",
    "\n",
    "max = counts.reduce(lambda r, c: getMax(r, c))\n",
    "\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for partitionCount in range(2, 9, 2):\n",
    "\n",
    "    rept = text.repartition(partitionCount)\n",
    "    print(rept.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 71744) partition count 2 time 469\n",
      "('the', 71744) partition count 4 time 434\n",
      "('the', 71744) partition count 6 time 482\n",
      "('the', 71744) partition count 8 time 465\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "from datetime import datetime\n",
    "\n",
    "# get the word with bigger count\n",
    "def getMax(r, c):\n",
    "  if (r[1] > c[1]):\n",
    "    return r\n",
    "  else:\n",
    "    return c\n",
    "\n",
    "file = \"/data/sherlock-holmes.txt\"\n",
    "\n",
    "# read text and cache\n",
    "text = sc.textFile(file).persist()\n",
    "\n",
    "minCores = 2\n",
    "numCores = 8\n",
    "\n",
    "# iterate over even number of partitions from minCores to numCores\n",
    "for partitionCount in range(minCores, numCores+1, 2):\n",
    "\n",
    "    rept = text.repartition(partitionCount)\n",
    "\n",
    "    dt1 = datetime.now()\n",
    "    words = rept.flatMap(lambda w : w.split(' '))\n",
    "    dict = words.map(lambda w : (w, 1))\n",
    "    counts = dict.reduceByKey(add)\n",
    "    max = counts.reduce(lambda r, c: getMax(r, c))\n",
    "    dt2 = datetime.now()\n",
    "    print(str(max) + \" partition count \" + str(partitionCount) + \" time \" + str(round((dt2-dt1).microseconds/1000)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
